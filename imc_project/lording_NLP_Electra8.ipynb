{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기존 encoding: {'input_ids': tensor([[    2, 33461,  4130,  4119,  4239,  4083,  4403,  4073,  4129, 13274,\n",
      "          5361,  2419,  4217,  4294, 20475,  6272,  4114,  4653,  4076,  8553,\n",
      "            18,     3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "기존 input_ids: tensor([[    2, 33461,  4130,  4119,  4239,  4083,  4403,  4073,  4129, 13274,\n",
      "          5361,  2419,  4217,  4294, 20475,  6272,  4114,  4653,  4076,  8553,\n",
      "            18,     3]])\n",
      "기존 attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tokens: ['CL', '##S', '##백', '##로', '##식', '##당', '##에', '##서', '갈비', '##찜', '두', '##개', '##랑', '비빔밥', '하나', '##시', '##켜', '##주', '##세요', '.']\n",
      "한글자 단위 토큰: ['C', 'L', 'S', '백', '로', '식', '당', '에', '서', '갈', '비', '찜', '두', '개', '랑', '비', '빔', '밥', '하', '나', '시', '켜', '주', '세', '요', '.']\n",
      "한글자 단위 input_ids: [39, 48, 55, 2744, 2575, 3009, 2360, 3130, 2914, 2014, 2818, 3415, 2419, 2027, 2531, 2818, 2822, 2739, 3755, 2236, 3008, 3541, 3323, 2926, 3183, 18]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch\n",
    "# 기존 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained('./koelectra_v3_ner_model4')\n",
    "\n",
    "# 입력 문장\n",
    "sentence = '백로식당에서 갈비찜 두개랑 비빔밥 하나시켜주세요.'\n",
    "sentence1 = '[CLS]' + sentence\n",
    "# 기본 토큰화 및 인코딩\n",
    "encoding = tokenizer(sentence1, return_tensors='pt', truncation=True, padding=True)\n",
    "\n",
    "# input_ids와 attention_mask 그대로 확인\n",
    "input_ids = encoding['input_ids']\n",
    "attention_mask = encoding['attention_mask']\n",
    "\n",
    "print(\"기존 encoding:\", encoding)\n",
    "print(\"기존 input_ids:\", input_ids)\n",
    "print(\"기존 attention_mask:\", attention_mask)\n",
    "\n",
    "# 기본 토큰화를 한글자 단위로 후처리\n",
    "tokens = tokenizer.tokenize(sentence1)\n",
    "list_text = []\n",
    "text_token = []\n",
    "print(\"tokens:\",tokens)\n",
    "for token in tokens:\n",
    "    if token.startswith(\"##\"):  # 특수 기호 제거\n",
    "        list_text[-1] += token[2:]\n",
    "    else:\n",
    "        list_text.append(list(token))  \n",
    "\n",
    "for a in list_text:\n",
    "    text_token.extend(a)\n",
    "\n",
    "# 한글자 단위 결과 출력\n",
    "print(\"한글자 단위 토큰:\", text_token)\n",
    "\n",
    "# input_ids를 한글자 단위로 다시 매핑 (필요 시 직접 ID를 생성)\n",
    "char_input_ids = [tokenizer.convert_tokens_to_ids(t) for t in text_token]\n",
    "print(\"한글자 단위 input_ids:\", char_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids를 torch 텐서로 변환\n",
    "char_input_ids = torch.tensor([char_input_ids])  # 2차원 텐서로 변환\n",
    "attention_mask = torch.ones_like(char_input_ids)  # 모든 토큰에 대해 마스크 생성\n",
    "\n",
    "# 저장된 모델 로드\n",
    "model = AutoModelForTokenClassification.from_pretrained('./koelectra_v3_ner_model4')\n",
    "model.eval()\n",
    "\n",
    "# 추론\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids=char_input_ids, attention_mask=attention_mask)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)  # 가장 높은 확률을 가진 클래스 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C', 'L', 'S', '백', '로', '식', '당', '에', '서', '갈', '비', '찜', '두', '개', '랑', '비', '빔', '밥', '하', '나', '시', '켜', '주', '세', '요', '.']\n",
      "tensor([6, 1, 2, 2, 2, 2, 2, 0, 0, 3, 4, 4, 5, 6, 0, 3, 4, 4, 5, 6, 0, 0, 0, 0,\n",
      "        0, 0])\n"
     ]
    }
   ],
   "source": [
    "print(text_token)\n",
    "print(predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_: 예측 6\n",
      "Top 3 클래스 확률:\n",
      "  LABEL_6: 0.4114\n",
      "  LABEL_2: 0.1874\n",
      "  LABEL_0: 0.1840\n",
      "\n",
      "백: 예측 1\n",
      "Top 3 클래스 확률:\n",
      "  LABEL_1: 0.9991\n",
      "  LABEL_2: 0.0002\n",
      "  LABEL_5: 0.0002\n",
      "\n",
      "로: 예측 2\n",
      "Top 3 클래스 확률:\n",
      "  LABEL_2: 0.9992\n",
      "  LABEL_4: 0.0003\n",
      "  LABEL_1: 0.0002\n",
      "\n",
      "식: 예측 2\n",
      "Top 3 클래스 확률:\n",
      "  LABEL_2: 0.9992\n",
      "  LABEL_4: 0.0002\n",
      "  LABEL_1: 0.0002\n",
      "\n",
      "당: 예측 2\n",
      "Top 3 클래스 확률:\n",
      "  LABEL_2: 0.9992\n",
      "  LABEL_4: 0.0002\n",
      "  LABEL_1: 0.0002\n",
      "\n",
      "에: 예측 0\n",
      "Top 3 클래스 확률:\n",
      "  LABEL_0: 0.9994\n",
      "  LABEL_6: 0.0001\n",
      "  LABEL_4: 0.0001\n",
      "\n",
      "서: 예측 0\n",
      "Top 3 클래스 확률:\n",
      "  LABEL_0: 0.9993\n",
      "  LABEL_6: 0.0002\n",
      "  LABEL_4: 0.0001\n",
      "\n",
      "갈: 예측 3\n",
      "Top 3 클래스 확률:\n",
      "  LABEL_3: 0.9986\n",
      "  LABEL_1: 0.0005\n",
      "  LABEL_2: 0.0003\n",
      "\n",
      "비: 예측 4\n",
      "Top 3 클래스 확률:\n",
      "  LABEL_4: 0.9987\n",
      "  LABEL_2: 0.0005\n",
      "  LABEL_1: 0.0002\n",
      "\n",
      "찜: 예측 4\n",
      "Top 3 클래스 확률:\n",
      "  LABEL_4: 0.9988\n",
      "  LABEL_2: 0.0005\n",
      "  LABEL_1: 0.0002\n",
      "\n",
      "두: 예측 5\n",
      "Top 3 클래스 확률:\n",
      "  LABEL_5: 0.9990\n",
      "  LABEL_6: 0.0002\n",
      "  LABEL_3: 0.0002\n",
      "\n",
      "개: 예측 6\n",
      "Top 3 클래스 확률:\n",
      "  LABEL_6: 0.9984\n",
      "  LABEL_0: 0.0006\n",
      "  LABEL_5: 0.0004\n",
      "\n",
      "랑: 예측 0\n",
      "Top 3 클래스 확률:\n",
      "  LABEL_0: 0.9994\n",
      "  LABEL_6: 0.0002\n",
      "  LABEL_4: 0.0001\n",
      "\n",
      "비: 예측 3\n",
      "Top 3 클래스 확률:\n",
      "  LABEL_3: 0.9984\n",
      "  LABEL_1: 0.0005\n",
      "  LABEL_4: 0.0003\n",
      "\n",
      "빔: 예측 4\n",
      "Top 3 클래스 확률:\n",
      "  LABEL_4: 0.9984\n",
      "  LABEL_2: 0.0004\n",
      "  LABEL_0: 0.0003\n",
      "\n",
      "밥: 예측 4\n",
      "Top 3 클래스 확률:\n",
      "  LABEL_4: 0.9988\n",
      "  LABEL_2: 0.0004\n",
      "  LABEL_1: 0.0002\n",
      "\n",
      "하: 예측 5\n",
      "Top 3 클래스 확률:\n",
      "  LABEL_5: 0.9988\n",
      "  LABEL_6: 0.0003\n",
      "  LABEL_3: 0.0003\n",
      "\n",
      "나: 예측 6\n",
      "Top 3 클래스 확률:\n",
      "  LABEL_6: 0.9988\n",
      "  LABEL_0: 0.0003\n",
      "  LABEL_2: 0.0002\n",
      "\n",
      "시: 예측 0\n",
      "Top 3 클래스 확률:\n",
      "  LABEL_0: 0.9995\n",
      "  LABEL_6: 0.0001\n",
      "  LABEL_2: 0.0001\n",
      "\n",
      "켜: 예측 0\n",
      "Top 3 클래스 확률:\n",
      "  LABEL_0: 0.9994\n",
      "  LABEL_4: 0.0001\n",
      "  LABEL_6: 0.0001\n",
      "\n",
      "주: 예측 0\n",
      "Top 3 클래스 확률:\n",
      "  LABEL_0: 0.9988\n",
      "  LABEL_2: 0.0005\n",
      "  LABEL_1: 0.0002\n",
      "\n",
      "세: 예측 0\n",
      "Top 3 클래스 확률:\n",
      "  LABEL_0: 0.9992\n",
      "  LABEL_6: 0.0002\n",
      "  LABEL_2: 0.0001\n",
      "\n",
      "요: 예측 0\n",
      "Top 3 클래스 확률:\n",
      "  LABEL_0: 0.9994\n",
      "  LABEL_6: 0.0001\n",
      "  LABEL_4: 0.0001\n",
      "\n",
      ".: 예측 0\n",
      "Top 3 클래스 확률:\n",
      "  LABEL_0: 0.9994\n",
      "  LABEL_6: 0.0001\n",
      "  LABEL_1: 0.0001\n",
      "\n"
     ]
    }
   ],
   "source": [
    " # 로짓 분석\n",
    "for i, (token, pred) in enumerate(zip(text_token, logits[0])):\n",
    "    # 각 클래스의 확률 분포 출력\n",
    "    probs = torch.softmax(pred, dim=0)\n",
    "    top_probs = probs.topk(3)\n",
    "    \n",
    "    print(f\"{token}: 예측 {predictions[0][i].item()}\")\n",
    "    print(\"Top 3 클래스 확률:\")\n",
    "    for value, index in zip(top_probs.values, top_probs.indices):\n",
    "        label = list(model.config.label2id.keys())[list(model.config.label2id.values()).index(index.item())]\n",
    "        print(f\"  {label}: {value.item():.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_tokenization(tokens, tokenizer):\n",
    "    \"\"\"토크나이징 과정 디버깅\"\"\"\n",
    "    print(\"원본 토큰:\", tokens)\n",
    "    \n",
    "    # 토큰화 결과 확인\n",
    "    encoded = tokenizer(\n",
    "        tokens, \n",
    "        is_split_into_words=True, \n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    # 단어 ID 출력\n",
    "    print(\"Word IDs:\", encoded.word_ids(batch_index=0))\n",
    "    \n",
    "    # 디코딩된 토큰 확인\n",
    "    decoded_tokens = tokenizer.convert_ids_to_tokens(encoded['input_ids'][0])\n",
    "    print(\"디코딩된 토큰:\", decoded_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

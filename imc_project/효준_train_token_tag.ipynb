{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "from EntityRecognition import *\n",
    "import torch.optim as optim\n",
    "\n",
    "model_name = 'klue/bert-base'\n",
    "num_labels = 7\n",
    "label_names = ['Other', 'B-store', 'I-store', 'B-menu', 'I-menu', 'B-count', 'I-count']\n",
    "\n",
    "# json 파일의 tokens와 ner_tags의 길이가 같은지 확인\n",
    "import pandas as pd \n",
    "a = pd.read_json('total_train_tag.json')\n",
    "for i, j in zip(a['tokens'], a['ner_tags']):\n",
    "    if len(i) == len(j):\n",
    "        pass\n",
    "    else:\n",
    "        print(False)\n",
    "        print(i)\n",
    "        print(j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "732ddf0cd71743fbb0332ae05f099def",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5010 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2815, 'grad_norm': 1.1423087120056152, 'learning_rate': 4.5009980039920164e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd6fab81cb9a43b1a7f9db45c9dcfe02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/126 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.10666277259588242, 'eval_f1': 0.9862025264170764, 'eval_accuracy': 0.9862528344671202, 'eval_runtime': 3.7571, 'eval_samples_per_second': 33.537, 'eval_steps_per_second': 33.537, 'epoch': 1.0}\n",
      "{'loss': 0.1087, 'grad_norm': 0.018871698528528214, 'learning_rate': 4.0019960079840326e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e1caba284de4c3091dd6cfbe671cdd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/126 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.08016765862703323, 'eval_f1': 0.9918177302511766, 'eval_accuracy': 0.9917800453514739, 'eval_runtime': 4.4528, 'eval_samples_per_second': 28.297, 'eval_steps_per_second': 28.297, 'epoch': 2.0}\n",
      "{'loss': 0.0665, 'grad_norm': 6.7953948974609375, 'learning_rate': 3.502994011976048e-05, 'epoch': 2.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85a8227373aa4801ab9daa68b410733a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/126 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.05527252331376076, 'eval_f1': 0.9931911606620722, 'eval_accuracy': 0.9931972789115646, 'eval_runtime': 3.5549, 'eval_samples_per_second': 35.444, 'eval_steps_per_second': 35.444, 'epoch': 3.0}\n",
      "{'loss': 0.0414, 'grad_norm': 0.15044212341308594, 'learning_rate': 3.003992015968064e-05, 'epoch': 3.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feaec3f2b4b44f38955d207c14112a6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/126 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.10680008679628372, 'eval_f1': 0.9928842129129135, 'eval_accuracy': 0.9929138321995464, 'eval_runtime': 3.2869, 'eval_samples_per_second': 38.334, 'eval_steps_per_second': 38.334, 'epoch': 4.0}\n",
      "{'loss': 0.0372, 'grad_norm': 0.0412713922560215, 'learning_rate': 2.5049900199600802e-05, 'epoch': 4.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91974312d2d24ea1816e12b7757452ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/126 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.06778990477323532, 'eval_f1': 0.9930429920512119, 'eval_accuracy': 0.9930555555555556, 'eval_runtime': 6.5425, 'eval_samples_per_second': 19.259, 'eval_steps_per_second': 19.259, 'epoch': 5.0}\n",
      "{'loss': 0.021, 'grad_norm': 0.0028368926141411066, 'learning_rate': 2.0059880239520957e-05, 'epoch': 5.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93820b4f4f4d4057a441a3d89ee1a0af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/126 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.07801984995603561, 'eval_f1': 0.9937784050773094, 'eval_accuracy': 0.9937641723356009, 'eval_runtime': 6.0277, 'eval_samples_per_second': 20.904, 'eval_steps_per_second': 20.904, 'epoch': 6.0}\n",
      "{'loss': 0.0144, 'grad_norm': 1.7763288021087646, 'learning_rate': 1.506986027944112e-05, 'epoch': 6.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdea73da16a64a9aa9065b75414afd89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/126 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.08232544362545013, 'eval_f1': 0.9951788216862615, 'eval_accuracy': 0.9951814058956916, 'eval_runtime': 7.3915, 'eval_samples_per_second': 17.047, 'eval_steps_per_second': 17.047, 'epoch': 7.0}\n",
      "{'loss': 0.0072, 'grad_norm': 0.006546113174408674, 'learning_rate': 1.0079840319361278e-05, 'epoch': 7.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c1203a798c44544b562337247f223a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/126 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.07533365488052368, 'eval_f1': 0.994746760914611, 'eval_accuracy': 0.9947562358276644, 'eval_runtime': 3.7542, 'eval_samples_per_second': 33.563, 'eval_steps_per_second': 33.563, 'epoch': 8.0}\n",
      "{'loss': 0.0056, 'grad_norm': 0.0008358720224350691, 'learning_rate': 5.0898203592814375e-06, 'epoch': 8.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a013a6b24da4c998db7106373da100e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/126 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.07572213560342789, 'eval_f1': 0.9948784575267294, 'eval_accuracy': 0.9948979591836735, 'eval_runtime': 8.1988, 'eval_samples_per_second': 15.368, 'eval_steps_per_second': 15.368, 'epoch': 9.0}\n",
      "{'loss': 0.0016, 'grad_norm': 0.0009197393083013594, 'learning_rate': 9.98003992015968e-08, 'epoch': 9.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5e4b7ad1ba44d1fbf659c0e1dc58676",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/126 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.07212182879447937, 'eval_f1': 0.9954600194721812, 'eval_accuracy': 0.9954648526077098, 'eval_runtime': 4.0412, 'eval_samples_per_second': 31.179, 'eval_steps_per_second': 31.179, 'epoch': 10.0}\n",
      "{'train_runtime': 2284.7257, 'train_samples_per_second': 2.193, 'train_steps_per_second': 2.193, 'train_loss': 0.058391266664491966, 'epoch': 10.0}\n"
     ]
    }
   ],
   "source": [
    "ner = NERTrainer(model_name, num_labels, label_names)\n",
    "\n",
    "optimizer = optim.AdamW(ner.model.parameters(), lr=5e-5)\n",
    "\n",
    "new_dataset = ner.make_new_dataset('total_train_tag.json')\n",
    "split_dataset = ner.split_training(new_dataset, optimizer, weight_decay = 0.01, test_size=0.01, batch_size=1, epoch=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 평가 지표"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36f476cfc59742edb4ca00c8e54b0f90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/126 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Other       1.00      1.00      1.00      5109\n",
      "     B-store       0.99      1.00      0.99       136\n",
      "     I-store       0.99      0.99      0.99       589\n",
      "      B-menu       0.98      0.98      0.98       199\n",
      "      I-menu       0.99      0.97      0.98       616\n",
      "     B-count       0.99      0.99      0.99       182\n",
      "     I-count       0.99      0.99      0.99       225\n",
      "\n",
      "    accuracy                           0.99      7056\n",
      "   macro avg       0.99      0.99      0.99      7056\n",
      "weighted avg       0.99      0.99      0.99      7056\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = ner.evaluate(split_dataset['test'])\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 테스트 데이터 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch\n",
    "from sklearn.metrics import *\n",
    "\n",
    "model_name = 'klue/bert-base'\n",
    "num_labels = 7\n",
    "label_names = ['Other', 'B-store', 'I-store', 'B-menu', 'I-menu', 'B-count', 'I-count']\n",
    "\n",
    "model_path = './model2/model'\n",
    "tokenizer_path = './model2/tokenizer'\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "test_tag = pd.read_json('total_test_tag.json')\n",
    "\n",
    "tokens = test_tag['tokens']\n",
    "ner_tags = test_tag['ner_tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9324950363997353\n",
      "f1_score : 0.9327029169159806\n",
      "mse : 0.5473196558570483\n"
     ]
    }
   ],
   "source": [
    "total_pred_list = []\n",
    "total_target_list = []\n",
    "\n",
    "for token, ner_tag in zip(tokens, ner_tags):\n",
    "    tokenized_inputs = tokenizer(token, truncation=True, is_split_into_words=True, return_tensors='pt')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(\n",
    "            input_ids = tokenized_inputs['input_ids'],\n",
    "            token_type_ids = tokenized_inputs['token_type_ids'],\n",
    "            attention_mask = tokenized_inputs['attention_mask']\n",
    "        )\n",
    "\n",
    "    pred = output.logits\n",
    "    pred_list = pred.argmax(dim=2).squeeze().tolist()[1:-1]\n",
    "\n",
    "    total_pred_list.extend(pred_list)\n",
    "    total_target_list.extend(ner_tag)\n",
    "\n",
    "accuracy = accuracy_score(total_target_list, total_pred_list)\n",
    "f1 = f1_score(total_target_list, total_pred_list, average='weighted')\n",
    "mse = mean_squared_error(total_target_list, total_pred_list)\n",
    "\n",
    "print(f\"accuracy : {accuracy}\")\n",
    "print(f\"f1_score : {f1}\")\n",
    "print(f\"mse : {mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "834381bb23894a9ebbed7e0e23a5be67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_pred = ner.pred_object(split_dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8927c6072c4e4a8d99f01ee7da42af65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_dataset = split_dataset['test']\n",
    "predictions, label_ids, metrics = ner.trainer.predict(test_dataset)\n",
    "\n",
    "preds = np.argmax(predictions, axis=2)\n",
    "tokens = test_dataset['tokens']\n",
    "tokenized_inputs = ner.tokenizer(tokens, is_split_into_words=True, truncation=True)\n",
    "input_ids = tokenized_inputs['input_ids']\n",
    "decode_tokens_list = []\n",
    "for i in input_ids:\n",
    "    decode_tokens = ner.tokenizer.convert_ids_to_tokens(i) # 인덱스 숫자 값을 다시 단어로 디코딩\n",
    "    decode_tokens_list.append(decode_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "매장명 : ['일 미리 금계 찜 닭']\n",
      "음식명 : ['닭볶음탕 중자', '떡 사리']\n",
      "수량 : ['하나', 'X']\n"
     ]
    }
   ],
   "source": [
    "def predict_entity(decode_token, pred):\n",
    "    STORE, FOOD, COUNT = [], [], []\n",
    "    for i in range(len(pred)):\n",
    "        if pred[i] == 1: STORE.append(decode_token[i])\n",
    "        elif pred[i] == 2: STORE[-1] += decode_token[i]\n",
    "        elif pred[i] == 3: FOOD.append(decode_token[i])\n",
    "        elif pred[i] == 4: FOOD[-1] += decode_token[i]\n",
    "        elif pred[i] == 5: COUNT.append(decode_token[i])\n",
    "        elif pred[i] == 6: COUNT[-1] += decode_token[i]\n",
    "\n",
    "    linear_text = ''.join(decode_token[1:-1])\n",
    "\n",
    "    food_start_index, count_start_index, min_index_list = [], [], []\n",
    "    for i in FOOD: food_start_index.append(linear_text.find(i))\n",
    "    for i in COUNT: count_start_index.append(linear_text.find(i))\n",
    "    for i in count_start_index:\n",
    "        diff_index = [i-j if i-j > 0 else 999 for j in food_start_index]\n",
    "        min_index = np.argmin(diff_index)\n",
    "        min_index_list.append(min_index)\n",
    "    min_index_list\n",
    "    new_count_list = ['X'] * len(FOOD)\n",
    "    for i, j in zip(COUNT, min_index_list): new_count_list[j] = i\n",
    "    \n",
    "    from konlpy.tag import Mecab\n",
    "    dicpath = '/opt/homebrew/lib/mecab/dic/mecab-ko-dic'\n",
    "    mecab = Mecab(dicpath + \" -r /opt/homebrew/etc/mecabrc\")\n",
    "\n",
    "    new_store, new_food, new_count = [], [], []\n",
    "    for i in STORE: new_store.append(' '.join(mecab.morphs(i)))\n",
    "    for i in FOOD: new_food.append(' '.join(mecab.morphs(i)))\n",
    "    for i in new_count_list: new_count.append(' '.join(mecab.morphs(i)))\n",
    "\n",
    "    return new_store, new_food, new_count\n",
    "\n",
    "index = 0\n",
    "decode_token = decode_tokens_list[index]\n",
    "pred = preds[index]\n",
    "store, food, count = predict_entity(decode_token, pred)\n",
    "\n",
    "print(f'매장명 : {store}')\n",
    "print(f'음식명 : {food}')\n",
    "print(f'수량 : {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_text(test_text):\n",
    "    test_token = [i for i in test_text.replace(' ', '')]\n",
    "    tokenized_inputs = ner.tokenizer(test_token, truncation=True, is_split_into_words=True)\n",
    "    data = {\n",
    "        'input_ids' : [tokenized_inputs['input_ids']],\n",
    "        'token_type_ids' : [tokenized_inputs['token_type_ids']],\n",
    "        'attention_mask' : [tokenized_inputs['attention_mask']]\n",
    "    }\n",
    "    input_dataset = Dataset.from_dict(data)\n",
    "    predictions, label_ids, metrics = ner.trainer.predict(input_dataset)\n",
    "\n",
    "    pred = np.argmax(predictions, axis=2)[0]\n",
    "    decode_tokens = [ner.tokenizer.convert_ids_to_tokens(i) for i in tokenized_inputs['input_ids']]\n",
    "\n",
    "    TOTAL_STORE, TOTAL_FOOD, TOTAL_COUNT = '', '', ''\n",
    "\n",
    "    zip_list = [i for i in zip(decode_tokens, pred) if i[1] != 0]\n",
    "    for i in range(len(zip_list)):\n",
    "        if i != len(zip_list) - 1:\n",
    "            if (zip_list[i][1] == 1) or (zip_list[i][1] == 2 and zip_list[i+1][1] == 2):\n",
    "                TOTAL_STORE += zip_list[i][0]\n",
    "            elif zip_list[i][1] == 2 and zip_list[i+1][1] != 2:\n",
    "                TOTAL_STORE += (zip_list[i][0] + ', ')\n",
    "            if (zip_list[i][1] == 3) or (zip_list[i][1] == 4 and zip_list[i+1][1] == 4):\n",
    "                TOTAL_FOOD += zip_list[i][0]\n",
    "            elif zip_list[i][1] == 4 and zip_list[i+1][1] != 4:\n",
    "                TOTAL_FOOD += (zip_list[i][0] + ', ')\n",
    "            if (zip_list[i][1] == 5) or (zip_list[i][1] == 6 and zip_list[i+1][1] == 6):\n",
    "                TOTAL_COUNT += zip_list[i][0]\n",
    "            elif zip_list[i][1] == 6 and zip_list[i+1][1] != 6:\n",
    "                TOTAL_COUNT += (zip_list[i][0] + ', ')\n",
    "        else:\n",
    "            if zip_list[i][1] == 1 or zip_list[i][1] == 2:\n",
    "                TOTAL_STORE += (zip_list[i][0] + ', ')\n",
    "            elif zip_list[i][1] == 3 or zip_list[i][1] == 4:\n",
    "                TOTAL_FOOD += (zip_list[i][0] + ', ')\n",
    "            elif zip_list[i][1] == 5 or zip_list[i][1] == 6:\n",
    "                TOTAL_COUNT += (zip_list[i][0] + ', ')\n",
    "\n",
    "    FINAL_STORE = TOTAL_STORE[:-2]\n",
    "    FINAL_FOOD = TOTAL_FOOD[:-2]\n",
    "    FINAL_COUNT = TOTAL_COUNT[:-2]\n",
    "\n",
    "    from konlpy.tag import Mecab\n",
    "\n",
    "    dicpath = '/opt/homebrew/lib/mecab/dic/mecab-ko-dic'\n",
    "    mecab = Mecab(dicpath + \" -r /opt/homebrew/etc/mecabrc\")\n",
    "\n",
    "    store_list = FINAL_STORE.split(', ')\n",
    "    food_list = FINAL_FOOD.split(', ')\n",
    "    count_list = FINAL_COUNT.split(', ')\n",
    "\n",
    "    new_store_list, new_food_list, new_count_list = [], [], []\n",
    "    for i in store_list:\n",
    "        mecab_token = ' '.join(mecab.morphs(i))\n",
    "        new_store_list.append(mecab_token)\n",
    "    for i in food_list:\n",
    "        mecab_token = ' '.join(mecab.morphs(i))\n",
    "        new_food_list.append(mecab_token)\n",
    "    for i in count_list:\n",
    "        mecab_token = ' '.join(mecab.morphs(i))\n",
    "        new_count_list.append(mecab_token)\n",
    "    \n",
    "    linear_text = test_text.replace(' ', '')\n",
    "    food_start_index = []\n",
    "    count_start_index = []\n",
    "    count_index = []\n",
    "\n",
    "    for i in new_food_list:\n",
    "        linear_food = i.replace(' ', '')\n",
    "        food_start_index.append(linear_text.find(linear_food))\n",
    "\n",
    "    for i in new_count_list:\n",
    "        linear_count = i.replace(' ', '')\n",
    "        count_start_index.append(linear_text.find(linear_count))\n",
    "\n",
    "    for i in count_start_index:\n",
    "        distance_list = [i-j for j in food_start_index]\n",
    "        plus_distance_list = [i if i > 0 else 9999 for i in distance_list]\n",
    "        min_value = min(plus_distance_list)\n",
    "        min_index = plus_distance_list.index(min_value)\n",
    "        count_index.append(min_index)\n",
    "\n",
    "    match_count = ['없음'] * len(new_food_list)\n",
    "    for index, i in enumerate(count_index):\n",
    "        match_count[i] = new_count_list[index]\n",
    "\n",
    "    return new_store_list, new_food_list, match_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47c27f74e41b464ca7c4ba13e5e5a6b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문 : BBQ에서 후라이드 양념 2마리씩 시킬게요\n",
      "매장명 : ['BBQ']\n",
      "음식명 : ['후라이드', '양념']\n",
      "수량 : ['없음', '2 마리 씩']\n"
     ]
    }
   ],
   "source": [
    "test_text = 'BBQ에서 후라이드 양념 2마리씩 시킬게요'\n",
    "store, food, count = predict_text(test_text)\n",
    "\n",
    "print(f'원문 : {test_text}')\n",
    "print(f'매장명 : {store}')\n",
    "print(f'음식명 : {food}')\n",
    "print(f'수량 : {count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텍스트 json에 넣기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_token_to_json(test_text, json_name):\n",
    "    tokens = [i for i in test_text if i != ' ']\n",
    "    string_tokens = str(tokens).replace(\"'\", '\"')\n",
    "    ner_tags = [0] * len(tokens)\n",
    "    with open(json_name, mode='r', encoding='utf-8') as f:\n",
    "        file = f.read()[:-2]\n",
    "\n",
    "    with open(json_name, mode='w', encoding='utf-8') as f:\n",
    "        f.write(file)\n",
    "        f.write(',\\n')\n",
    "        f.write('    {\\n')\n",
    "        f.write(f'        \"tokens\" : {string_tokens},\\n')\n",
    "        f.write(f'        \"ner_tags\" : {ner_tags}\\n')\n",
    "        f.write('    }\\n')\n",
    "        f.write(']')\n",
    "\n",
    "input_text = '짜장천국에서 탕수육 2인 세트 시키는데 메뉴는 중화 비빔밥이랑 짬뽕으로 할게'\n",
    "insert_token_to_json(input_text, 'total_train_tag.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 성윤님 데이터 옮기기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_token_to_json(test_text, ner_string, json_name):\n",
    "    tokens = [i for i in test_text if i != ' ']\n",
    "    string_tokens = str(tokens).replace(\"'\", '\"')\n",
    "    ner_tags = [int(i) for i in ner_string]\n",
    "    with open(json_name, mode='r', encoding='utf-8') as f:\n",
    "        file = f.read()[:-2]\n",
    "\n",
    "    with open(json_name, mode='w', encoding='utf-8') as f:\n",
    "        f.write(file)\n",
    "        f.write(',\\n')\n",
    "        f.write('    {\\n')\n",
    "        f.write(f'        \"tokens\" : {string_tokens},\\n')\n",
    "        f.write(f'        \"ner_tags\" : {ner_tags}\\n')\n",
    "        f.write('    }\\n')\n",
    "        f.write(']')\n",
    "\n",
    "df = pd.read_json('character_test.json')\n",
    "tokens = df['tokens']\n",
    "ner_tags = df['ner_tags']\n",
    "\n",
    "new_tokens = []\n",
    "new_ner_tags = []\n",
    "\n",
    "for i in tokens:\n",
    "    new_tokens.append(''.join(i))\n",
    "for i in ner_tags:\n",
    "    new_ner_tags.append(''.join(i))\n",
    "\n",
    "for i in range(len(new_tokens)):\n",
    "    insert_token_to_json(new_tokens[i], new_ner_tags[i], 'total_test_tag.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 불러와서 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_text_from_model(test_text, model, tokenizer):\n",
    "    test_token = [i for i in test_text.replace(' ', '')]\n",
    "    tokenized_inputs = tokenizer(test_token, truncation=True, is_split_into_words=True, return_tensors='pt')\n",
    "\n",
    "    import torch\n",
    "    # 모델로 추론\n",
    "    with torch.no_grad():\n",
    "        output = model(\n",
    "            input_ids = tokenized_inputs['input_ids'],\n",
    "            token_type_ids = tokenized_inputs['token_type_ids'],\n",
    "            attention_mask = tokenized_inputs['attention_mask']\n",
    "        )\n",
    "\n",
    "    predictions = output.logits # 각 글자의 0~6 클래스 확률값 출력\n",
    "\n",
    "    pred = torch.argmax(predictions, dim=2).squeeze().tolist()\n",
    "    decode_tokens = [tokenizer.convert_ids_to_tokens(i) for i in tokenized_inputs['input_ids']][0]\n",
    "\n",
    "    TOTAL_STORE, TOTAL_FOOD, TOTAL_COUNT = '', '', ''\n",
    "\n",
    "    zip_list = [i for i in zip(decode_tokens, pred) if i[1] != 0]\n",
    "    for i in range(len(zip_list)):\n",
    "        if i != len(zip_list) - 1:\n",
    "            if (zip_list[i][1] == 1) or (zip_list[i][1] == 2 and zip_list[i+1][1] == 2):\n",
    "                TOTAL_STORE += zip_list[i][0]\n",
    "            elif zip_list[i][1] == 2 and zip_list[i+1][1] != 2:\n",
    "                TOTAL_STORE += (zip_list[i][0] + ', ')\n",
    "            if (zip_list[i][1] == 3) or (zip_list[i][1] == 4 and zip_list[i+1][1] == 4):\n",
    "                TOTAL_FOOD += zip_list[i][0]\n",
    "            elif zip_list[i][1] == 4 and zip_list[i+1][1] != 4:\n",
    "                TOTAL_FOOD += (zip_list[i][0] + ', ')\n",
    "            if (zip_list[i][1] == 5) or (zip_list[i][1] == 6 and zip_list[i+1][1] == 6):\n",
    "                TOTAL_COUNT += zip_list[i][0]\n",
    "            elif zip_list[i][1] == 6 and zip_list[i+1][1] != 6:\n",
    "                TOTAL_COUNT += (zip_list[i][0] + ', ')\n",
    "        else:\n",
    "            if zip_list[i][1] == 1 or zip_list[i][1] == 2:\n",
    "                TOTAL_STORE += (zip_list[i][0] + ', ')\n",
    "            elif zip_list[i][1] == 3 or zip_list[i][1] == 4:\n",
    "                TOTAL_FOOD += (zip_list[i][0] + ', ')\n",
    "            elif zip_list[i][1] == 5 or zip_list[i][1] == 6:\n",
    "                TOTAL_COUNT += (zip_list[i][0] + ', ')\n",
    "\n",
    "    FINAL_STORE = TOTAL_STORE[:-2]\n",
    "    FINAL_FOOD = TOTAL_FOOD[:-2]\n",
    "    FINAL_COUNT = TOTAL_COUNT[:-2]\n",
    "\n",
    "    store_list = FINAL_STORE.split(', ')\n",
    "    food_list = FINAL_FOOD.split(', ')\n",
    "    count_list = FINAL_COUNT.split(', ')\n",
    "    \n",
    "    linear_text = test_text.replace(' ', '')\n",
    "    food_start_index = []\n",
    "    count_start_index = []\n",
    "    count_index = []\n",
    "\n",
    "    for i in food_list:\n",
    "        food_start_index.append(linear_text.find(i))\n",
    "\n",
    "    for i in count_list:\n",
    "        count_start_index.append(linear_text.find(i))\n",
    "\n",
    "    for i in count_start_index:\n",
    "        distance_list = [i-j for j in food_start_index]\n",
    "        plus_distance_list = [i if i > 0 else 9999 for i in distance_list]\n",
    "        min_value = min(plus_distance_list)\n",
    "        min_index = plus_distance_list.index(min_value)\n",
    "        count_index.append(min_index)\n",
    "\n",
    "    match_count = [''] * len(food_list)\n",
    "    for index, i in enumerate(count_index):\n",
    "        match_count[i] = count_list[index]\n",
    "    \n",
    "    def find_text(order_text, entity):\n",
    "        index_list = []\n",
    "        used_word_list = []\n",
    "        for i in entity:\n",
    "            count = used_word_list.count(i)\n",
    "            index = -1\n",
    "            if count >= 1:\n",
    "                for _ in range(count):\n",
    "                    index = order_text.find(i, index+1)\n",
    "            index_list.append(order_text.find(i, index+1))\n",
    "            used_word_list.append(i)\n",
    "        string = ''\n",
    "        for i in range(len(index_list)):\n",
    "            if i != len(index_list)-1:\n",
    "                if (index_list[i] + 1 == index_list[i+1]):\n",
    "                    string += order_text[index_list[i]]\n",
    "                else:\n",
    "                    string += order_text[index_list[i]] + ' '\n",
    "            else:\n",
    "                string += order_text[index_list[i]]\n",
    "        return string\n",
    "\n",
    "\n",
    "    copy_text = str(test_text) # 문자열 복사\n",
    "    entity = linear_text\n",
    "    text_tag = pred[1:-1]\n",
    "\n",
    "    origin_tag = []\n",
    "    for i in range(len(copy_text)):\n",
    "        if entity[0] == copy_text[0]:\n",
    "            copy_text = copy_text[1:]\n",
    "            entity = entity[1:]\n",
    "            origin_tag.append(text_tag.pop(0))\n",
    "        else:\n",
    "            copy_text = copy_text[1:]\n",
    "            origin_tag.append(0)\n",
    "\n",
    "    entity_text = []\n",
    "    for index, i in enumerate(origin_tag):\n",
    "        if i != 0:\n",
    "            entity_text.append(test_text[index])\n",
    "        else:\n",
    "            entity_text.append(' ')\n",
    "\n",
    "    import re\n",
    "    final_entity_text = re.sub(r'\\s{2,}', ' ', ''.join(entity_text)).rstrip()\n",
    "\n",
    "\n",
    "    new_store_list = []\n",
    "    for i in store_list:\n",
    "        new_store_list.append(find_text(final_entity_text, i))\n",
    "        final_entity_text = final_entity_text.replace(find_text(final_entity_text, i), '')\n",
    "    \n",
    "    new_food_list = []\n",
    "    for i in food_list:\n",
    "        new_food_list.append(find_text(final_entity_text, i))\n",
    "        final_entity_text = final_entity_text.replace(find_text(final_entity_text, i), '')\n",
    "        \n",
    "    new_count_list = []\n",
    "    for i in match_count:\n",
    "        new_count_list.append(find_text(final_entity_text, i))\n",
    "        final_entity_text = final_entity_text.replace(find_text(final_entity_text, i), '')\n",
    "\n",
    "    # 씩 단어 처리\n",
    "    empty_index = []\n",
    "    for index, i in enumerate(new_count_list):\n",
    "        if i == '':\n",
    "            empty_index.append(index)\n",
    "        elif '씩' in i:\n",
    "            for j in empty_index:\n",
    "                new_count_list[j] = i.replace('씩', '').strip()\n",
    "                new_count_list[index] = i.replace('씩', '').strip()\n",
    "                empty_index = []\n",
    "\n",
    "    return new_store_list, new_food_list, new_count_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문 : 롯데리아에서 데리버거 1개 새우버거랑 불고기버거는 2개씩 주시고 티렉스버거랑 행운버거는 3개씩 주세요\n",
      "매장명 : ['롯데리아']\n",
      "음식명 : ['데리버거', '새우버거', '불고기버거', '티렉스버거', '행운버거']\n",
      "수량 : ['1개', '2개', '2개', '3개', '3개']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "# 저장 경로\n",
    "model_path = \"./model1/model\"\n",
    "tokenizer_path = \"./model1/tokenizer\"\n",
    "\n",
    "# 모델과 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
    "\n",
    "test_text = '롯데리아에서 데리버거 1개 새우버거랑 불고기버거는 2개씩 주시고 티렉스버거랑 행운버거는 3개씩 주세요'\n",
    "store, food, count = predict_text_from_model(test_text, model, tokenizer)\n",
    "\n",
    "print(f'원문 : {test_text}')\n",
    "print(f'매장명 : {store}')\n",
    "print(f'음식명 : {food}')\n",
    "print(f'수량 : {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_index = []\n",
    "for index, i in enumerate(count):\n",
    "    if i == '':\n",
    "        empty_index.append(index)\n",
    "    elif '씩' in i:\n",
    "        for j in empty_index:\n",
    "            count[j] = i.replace('씩', '').strip()\n",
    "            count[index] = i.replace('씩', '').strip()\n",
    "            empty_index = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'스타벅스 린스'"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_text(order_text, entity):\n",
    "    index_list = []\n",
    "    used_word_list = []\n",
    "    for i in entity:\n",
    "        count = used_word_list.count(i)\n",
    "        index = -1\n",
    "        if count >= 1:\n",
    "            for _ in range(count):\n",
    "                index = order_text.find(i, index+1)\n",
    "        index_list.append(order_text.find(i, index+1))\n",
    "        used_word_list.append(i)\n",
    "    string = ''\n",
    "    for i in range(len(index_list)):\n",
    "        if i != len(index_list)-1:\n",
    "            if (index_list[i] + 1 == index_list[i+1]):\n",
    "                string += order_text[index_list[i]]\n",
    "            else:\n",
    "                string += order_text[index_list[i]] + ' '\n",
    "        else:\n",
    "            string += order_text[index_list[i]]\n",
    "    return string\n",
    "\n",
    "test_text = '스타벅스 린스에서 불고기 고기버거 2개랑 새우버거 3개 주문해줘'\n",
    "store = '스타벅스 린스'\n",
    "\n",
    "find_text(test_text, store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 테스트 데이터 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch\n",
    "from sklearn.metrics import *\n",
    "\n",
    "model_name = 'klue/bert-base'\n",
    "num_labels = 7\n",
    "label_names = ['Other', 'B-store', 'I-store', 'B-menu', 'I-menu', 'B-count', 'I-count']\n",
    "\n",
    "model_path = './results/model'\n",
    "tokenizer_path = './results/tokenizer'\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "test_tag = pd.read_json('total_test_tag.json')\n",
    "\n",
    "tokens = test_tag['tokens']\n",
    "ner_tags = test_tag['ner_tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9272005294506949\n",
      "f1_score : 0.9271815295802844\n",
      "mse : 0.6578424884182661\n"
     ]
    }
   ],
   "source": [
    "total_pred_list = []\n",
    "total_target_list = []\n",
    "\n",
    "for token, ner_tag in zip(tokens, ner_tags):\n",
    "    tokenized_inputs = tokenizer(token, truncation=True, is_split_into_words=True, return_tensors='pt')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(\n",
    "            input_ids = tokenized_inputs['input_ids'],\n",
    "            token_type_ids = tokenized_inputs['token_type_ids'],\n",
    "            attention_mask = tokenized_inputs['attention_mask']\n",
    "        )\n",
    "\n",
    "    pred = output.logits\n",
    "    pred_list = pred.argmax(dim=2).squeeze().tolist()[1:-1]\n",
    "\n",
    "    total_pred_list.extend(pred_list)\n",
    "    total_target_list.extend(ner_tag)\n",
    "\n",
    "accuracy = accuracy_score(total_target_list, total_pred_list)\n",
    "f1 = f1_score(total_target_list, total_pred_list, average='weighted')\n",
    "mse = mean_squared_error(total_target_list, total_pred_list)\n",
    "\n",
    "print(f\"accuracy : {accuracy}\")\n",
    "print(f\"f1_score : {f1}\")\n",
    "print(f\"mse : {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델이 작동하는 방식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "# 모델과 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
    "\n",
    "test_token = [i for i in test_text.replace(' ', '')]\n",
    "tokenized_inputs = tokenizer(test_token, truncation=True, is_split_into_words=True, return_tensors='pt')\n",
    "import torch\n",
    "# 모델로 추론\n",
    "with torch.no_grad():\n",
    "    output = model(\n",
    "        input_ids = tokenized_inputs['input_ids'],\n",
    "        token_type_ids = tokenized_inputs['token_type_ids'],\n",
    "        attention_mask = tokenized_inputs['attention_mask']\n",
    "    )\n",
    "\n",
    "predictions = output.logits # 각 글자의 0~6 클래스 확률값 출력\n",
    "\n",
    "pred = torch.argmax(predictions, dim=2).squeeze().tolist()\n",
    "decode_tokens = [tokenizer.convert_ids_to_tokens(i) for i in tokenized_inputs['input_ids']][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = output.logits\n",
    "preds = np.argmax(logits, axis=2).squeeze().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 2, 2, 0, 0, 3, 4, 4, 4, 4, 4, 4, 4, 5, 6, 0, 0, 3, 4, 4, 4, 5, 6, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '카', '페', '베', '네', '에', '서', '아', '이', '스', '아', '메', '리', '카', '노', '한', '잔', '이', '랑', '카', '페', '모', '카', '한', '잔', '주', '세', '요', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(decode_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "model_path = './results/model'\n",
    "tokenizer_path = './results/tokenizer'\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "pipe = pipeline('ner', model=model, tokenizer=tokenizer, aggregation_strategy='simple', device=0)\n",
    "text = '맥도날드에서 행운 버거 3개랑 빅맥 5개 그리고 콜라 3개랑 사이다 2개 포장해줘'\n",
    "result = pipe(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TORCH_TEXT_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 모듈 로딩\n",
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import imantics\n",
    "from PIL import Image\n",
    "from skimage.transform import resize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from torch import optim\n",
    "from torchmetrics.classification import *\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.nn.functional as F\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이름 변경"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0은 충치없음 1은 충치있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = './data/0_and_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing = transforms.Compose([\n",
    "transforms.Resize((512,512)),\n",
    "transforms.ToTensor()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgDS = datasets.ImageFolder(root=img_dir, transform=preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': 0, '1': 1}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgDS.class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, val, test 분할 비율 정의 (예: 70%, 15%, 15%)\n",
    "train_size = int(0.7 * len(imgDS))\n",
    "val_size = int(0.15 * len(imgDS))\n",
    "test_size = len(imgDS) - train_size - val_size\n",
    "\n",
    "# 데이터 분할\n",
    "train_data, val_data, test_data = random_split(imgDS, [train_size, val_size, test_size])\n",
    "\n",
    "# DataLoader 생성 (batch_size는 필요에 따라 설정)\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140 30 30\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data),len(val_data),len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "from torchvision.models import VGG16_Weights\n",
    "from torchinfo import summary\n",
    "\n",
    "# 사전 학습된 모델 로딩\n",
    "model = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "VGG                                      [1, 1000]                 --\n",
       "├─Sequential: 1-1                        [1, 512, 16, 16]          --\n",
       "│    └─Conv2d: 2-1                       [1, 64, 512, 512]         1,792\n",
       "│    └─ReLU: 2-2                         [1, 64, 512, 512]         --\n",
       "│    └─Conv2d: 2-3                       [1, 64, 512, 512]         36,928\n",
       "│    └─ReLU: 2-4                         [1, 64, 512, 512]         --\n",
       "│    └─MaxPool2d: 2-5                    [1, 64, 256, 256]         --\n",
       "│    └─Conv2d: 2-6                       [1, 128, 256, 256]        73,856\n",
       "│    └─ReLU: 2-7                         [1, 128, 256, 256]        --\n",
       "│    └─Conv2d: 2-8                       [1, 128, 256, 256]        147,584\n",
       "│    └─ReLU: 2-9                         [1, 128, 256, 256]        --\n",
       "│    └─MaxPool2d: 2-10                   [1, 128, 128, 128]        --\n",
       "│    └─Conv2d: 2-11                      [1, 256, 128, 128]        295,168\n",
       "│    └─ReLU: 2-12                        [1, 256, 128, 128]        --\n",
       "│    └─Conv2d: 2-13                      [1, 256, 128, 128]        590,080\n",
       "│    └─ReLU: 2-14                        [1, 256, 128, 128]        --\n",
       "│    └─Conv2d: 2-15                      [1, 256, 128, 128]        590,080\n",
       "│    └─ReLU: 2-16                        [1, 256, 128, 128]        --\n",
       "│    └─MaxPool2d: 2-17                   [1, 256, 64, 64]          --\n",
       "│    └─Conv2d: 2-18                      [1, 512, 64, 64]          1,180,160\n",
       "│    └─ReLU: 2-19                        [1, 512, 64, 64]          --\n",
       "│    └─Conv2d: 2-20                      [1, 512, 64, 64]          2,359,808\n",
       "│    └─ReLU: 2-21                        [1, 512, 64, 64]          --\n",
       "│    └─Conv2d: 2-22                      [1, 512, 64, 64]          2,359,808\n",
       "│    └─ReLU: 2-23                        [1, 512, 64, 64]          --\n",
       "│    └─MaxPool2d: 2-24                   [1, 512, 32, 32]          --\n",
       "│    └─Conv2d: 2-25                      [1, 512, 32, 32]          2,359,808\n",
       "│    └─ReLU: 2-26                        [1, 512, 32, 32]          --\n",
       "│    └─Conv2d: 2-27                      [1, 512, 32, 32]          2,359,808\n",
       "│    └─ReLU: 2-28                        [1, 512, 32, 32]          --\n",
       "│    └─Conv2d: 2-29                      [1, 512, 32, 32]          2,359,808\n",
       "│    └─ReLU: 2-30                        [1, 512, 32, 32]          --\n",
       "│    └─MaxPool2d: 2-31                   [1, 512, 16, 16]          --\n",
       "├─AdaptiveAvgPool2d: 1-2                 [1, 512, 7, 7]            --\n",
       "├─Sequential: 1-3                        [1, 1000]                 --\n",
       "│    └─Linear: 2-32                      [1, 4096]                 102,764,544\n",
       "│    └─ReLU: 2-33                        [1, 4096]                 --\n",
       "│    └─Dropout: 2-34                     [1, 4096]                 --\n",
       "│    └─Linear: 2-35                      [1, 4096]                 16,781,312\n",
       "│    └─ReLU: 2-36                        [1, 4096]                 --\n",
       "│    └─Dropout: 2-37                     [1, 4096]                 --\n",
       "│    └─Linear: 2-38                      [1, 1000]                 4,097,000\n",
       "==========================================================================================\n",
       "Total params: 138,357,544\n",
       "Trainable params: 138,357,544\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 80.37\n",
       "==========================================================================================\n",
       "Input size (MB): 3.15\n",
       "Forward/backward pass size (MB): 566.30\n",
       "Params size (MB): 553.43\n",
       "Estimated Total Size (MB): 1122.88\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model, input_size=[1, 3, 512, 512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomVgg16Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomVgg16Model, self).__init__()    \n",
    "        self.features = model.features\n",
    "        self.avgpool = model.avgpool\n",
    "        self.classifier = model.classifier\n",
    "        self.custom_layer = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1000, 500),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = self.features(x)\n",
    "        y = self.avgpool(y)\n",
    "        y = torch.flatten(y, 1)\n",
    "        y = self.classifier(y)\n",
    "        y = F.sigmoid(self.custom_layer(y))\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomVgg16Model(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      "  (custom_layer): Sequential(\n",
      "    (0): ReLU()\n",
      "    (1): Linear(in_features=1000, out_features=500, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=500, out_features=50, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "CustomVgg16Model                         --\n",
       "├─Sequential: 1-1                        --\n",
       "│    └─Conv2d: 2-1                       1,792\n",
       "│    └─ReLU: 2-2                         --\n",
       "│    └─Conv2d: 2-3                       36,928\n",
       "│    └─ReLU: 2-4                         --\n",
       "│    └─MaxPool2d: 2-5                    --\n",
       "│    └─Conv2d: 2-6                       73,856\n",
       "│    └─ReLU: 2-7                         --\n",
       "│    └─Conv2d: 2-8                       147,584\n",
       "│    └─ReLU: 2-9                         --\n",
       "│    └─MaxPool2d: 2-10                   --\n",
       "│    └─Conv2d: 2-11                      295,168\n",
       "│    └─ReLU: 2-12                        --\n",
       "│    └─Conv2d: 2-13                      590,080\n",
       "│    └─ReLU: 2-14                        --\n",
       "│    └─Conv2d: 2-15                      590,080\n",
       "│    └─ReLU: 2-16                        --\n",
       "│    └─MaxPool2d: 2-17                   --\n",
       "│    └─Conv2d: 2-18                      1,180,160\n",
       "│    └─ReLU: 2-19                        --\n",
       "│    └─Conv2d: 2-20                      2,359,808\n",
       "│    └─ReLU: 2-21                        --\n",
       "│    └─Conv2d: 2-22                      2,359,808\n",
       "│    └─ReLU: 2-23                        --\n",
       "│    └─MaxPool2d: 2-24                   --\n",
       "│    └─Conv2d: 2-25                      2,359,808\n",
       "│    └─ReLU: 2-26                        --\n",
       "│    └─Conv2d: 2-27                      2,359,808\n",
       "│    └─ReLU: 2-28                        --\n",
       "│    └─Conv2d: 2-29                      2,359,808\n",
       "│    └─ReLU: 2-30                        --\n",
       "│    └─MaxPool2d: 2-31                   --\n",
       "├─AdaptiveAvgPool2d: 1-2                 --\n",
       "├─Sequential: 1-3                        --\n",
       "│    └─Linear: 2-32                      102,764,544\n",
       "│    └─ReLU: 2-33                        --\n",
       "│    └─Dropout: 2-34                     --\n",
       "│    └─Linear: 2-35                      16,781,312\n",
       "│    └─ReLU: 2-36                        --\n",
       "│    └─Dropout: 2-37                     --\n",
       "│    └─Linear: 2-38                      4,097,000\n",
       "├─Sequential: 1-4                        --\n",
       "│    └─ReLU: 2-39                        --\n",
       "│    └─Linear: 2-40                      500,500\n",
       "│    └─ReLU: 2-41                        --\n",
       "│    └─Linear: 2-42                      25,050\n",
       "│    └─ReLU: 2-43                        --\n",
       "│    └─Linear: 2-44                      51\n",
       "=================================================================\n",
       "Total params: 138,883,145\n",
       "Trainable params: 138,883,145\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CustomVgg16Model()\n",
    "print(model)\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[features.0.weight] - torch.Size([64, 3, 3, 3])\n",
      "[features.0.bias] - torch.Size([64])\n",
      "[features.2.weight] - torch.Size([64, 64, 3, 3])\n",
      "[features.2.bias] - torch.Size([64])\n",
      "[features.5.weight] - torch.Size([128, 64, 3, 3])\n",
      "[features.5.bias] - torch.Size([128])\n",
      "[features.7.weight] - torch.Size([128, 128, 3, 3])\n",
      "[features.7.bias] - torch.Size([128])\n",
      "[features.10.weight] - torch.Size([256, 128, 3, 3])\n",
      "[features.10.bias] - torch.Size([256])\n",
      "[features.12.weight] - torch.Size([256, 256, 3, 3])\n",
      "[features.12.bias] - torch.Size([256])\n",
      "[features.14.weight] - torch.Size([256, 256, 3, 3])\n",
      "[features.14.bias] - torch.Size([256])\n",
      "[features.17.weight] - torch.Size([512, 256, 3, 3])\n",
      "[features.17.bias] - torch.Size([512])\n",
      "[features.19.weight] - torch.Size([512, 512, 3, 3])\n",
      "[features.19.bias] - torch.Size([512])\n",
      "[features.21.weight] - torch.Size([512, 512, 3, 3])\n",
      "[features.21.bias] - torch.Size([512])\n",
      "[features.24.weight] - torch.Size([512, 512, 3, 3])\n",
      "[features.24.bias] - torch.Size([512])\n",
      "[features.26.weight] - torch.Size([512, 512, 3, 3])\n",
      "[features.26.bias] - torch.Size([512])\n",
      "[features.28.weight] - torch.Size([512, 512, 3, 3])\n",
      "[features.28.bias] - torch.Size([512])\n",
      "[classifier.0.weight] - torch.Size([4096, 25088])\n",
      "[classifier.0.bias] - torch.Size([4096])\n",
      "[classifier.3.weight] - torch.Size([4096, 4096])\n",
      "[classifier.3.bias] - torch.Size([4096])\n",
      "[classifier.6.weight] - torch.Size([1000, 4096])\n",
      "[classifier.6.bias] - torch.Size([1000])\n",
      "[custom_layer.1.weight] - torch.Size([500, 1000])\n",
      "[custom_layer.1.bias] - torch.Size([500])\n",
      "[custom_layer.3.weight] - torch.Size([50, 500])\n",
      "[custom_layer.3.bias] - torch.Size([50])\n",
      "[custom_layer.5.weight] - torch.Size([1, 50])\n",
      "[custom_layer.5.bias] - torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "# 사전학습된 모델의 파라미터 비활성화 설정\n",
    "for named, params in model.named_parameters():\n",
    "    print(f\"[{named}] - {params.shape}\")\n",
    "    # 역전파시 업데이트가 되지 않도록 설정\n",
    "    params.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.weight] - torch.Size([500, 1000])\n",
      "[1.bias] - torch.Size([500])\n",
      "[3.weight] - torch.Size([50, 500])\n",
      "[3.bias] - torch.Size([50])\n",
      "[5.weight] - torch.Size([1, 50])\n",
      "[5.bias] - torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "# 모델의 FC 파라미터 활성화 설정\n",
    "for named, params in model.custom_layer.named_parameters():\n",
    "    print(f\"[{named}] - {params.shape}\")\n",
    "    params.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 학습 진행 관련 설정\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")\n",
    "epochs = 100\n",
    "BATCH_SIZE = 8\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "LR = 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "crossLoss = nn.BCELoss()\n",
    "# scheduler = ReduceLROnPlateau(optimizer, patience=10, mode='max')\n",
    "criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "train_batch_cnt = len(train_loader) / BATCH_SIZE\n",
    "val_batch_cnt = len(val_loader) / BATCH_SIZE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataset, criterion, optimizer, device, interval):\n",
    "    model.train()\n",
    "    losses = list()\n",
    "    corrects = 0\n",
    "    total = 0\n",
    "\n",
    "    for step, (images, labels) in enumerate(dataset):\n",
    "        images, labels = images.to(device), labels.to(device).float().unsqueeze(1)\n",
    "   \n",
    "        pre_y = model(images)\n",
    "    \n",
    "        loss = criterion(pre_y, labels)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        yhat = torch.sigmoid(pre_y)>.5\n",
    "        corrects += torch.eq(yhat, labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % interval == 0:\n",
    "            print(f'Train Loss {step} : {np.mean(losses)}, Train Accuracy: {corrects / total:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Val(model, dataset, criterion, device, epoch):\n",
    "    model.eval()\n",
    "    losses_V= list()\n",
    "    corrects_V = list()\n",
    "\n",
    "    for images, labels in dataset:\n",
    "        images, labels = images.to(device), labels.to(device).float().unsqueeze(1)\n",
    "        \n",
    "        pre_v = model(images)\n",
    "        loss = criterion(pre_v, labels)\n",
    "        losses_V.append(loss.item())\n",
    "        \n",
    "        yhat = torch.sigmoid(pre_v)>.5\n",
    "        corrects_V.extend(torch.eq(yhat, labels).cpu().tolist())\n",
    "        \n",
    "    val_loss = np.mean(losses_V)\n",
    "    val_accuracy = np.mean(corrects_V)\n",
    "\n",
    "    print(f'{epoch} Val Loss : {val_loss:.4f}, Val Accuracy : {val_accuracy:.4f}')\n",
    "\n",
    "    return val_loss, val_accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss 0 : 0.7361971735954285, Train Accuracy: 0.4688\n",
      "0 Val Loss : 0.5921, Val Accuracy : 0.5357\n",
      "Train Loss 0 : 0.5415087938308716, Train Accuracy: 0.6875\n",
      "1 Val Loss : 0.5216, Val Accuracy : 0.5357\n",
      "Train Loss 0 : 0.5575911402702332, Train Accuracy: 0.4375\n",
      "2 Val Loss : 0.4993, Val Accuracy : 0.5357\n",
      "Train Loss 0 : 0.49721765518188477, Train Accuracy: 0.5312\n",
      "3 Val Loss : 0.4870, Val Accuracy : 0.5357\n",
      "Train Loss 0 : 0.4725596010684967, Train Accuracy: 0.5938\n",
      "4 Val Loss : 0.5128, Val Accuracy : 0.5357\n",
      "Train Loss 0 : 0.5211254358291626, Train Accuracy: 0.5312\n",
      "5 Val Loss : 0.5009, Val Accuracy : 0.5429\n",
      "Train Loss 0 : 0.4449757933616638, Train Accuracy: 0.7188\n",
      "6 Val Loss : 0.4886, Val Accuracy : 0.5357\n",
      "Train Loss 0 : 0.47077617049217224, Train Accuracy: 0.5938\n",
      "7 Val Loss : 0.4879, Val Accuracy : 0.5357\n",
      "Train Loss 0 : 0.5439552664756775, Train Accuracy: 0.4375\n",
      "8 Val Loss : 0.4848, Val Accuracy : 0.5786\n",
      "Train Loss 0 : 0.5131014585494995, Train Accuracy: 0.6250\n",
      "9 Val Loss : 0.4851, Val Accuracy : 0.5857\n",
      "Train Loss 0 : 0.48003605008125305, Train Accuracy: 0.5938\n",
      "10 Val Loss : 0.4895, Val Accuracy : 0.6357\n"
     ]
    }
   ],
   "source": [
    "interval = 10\n",
    "BEST_HISTORY = 0\n",
    "all_score = []\n",
    "all_loss = []\n",
    "stop_num = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train(model, train_loader, criterion, optimizer, device, interval)\n",
    "    loss, score = Val(model, train_loader, criterion, device, epoch)\n",
    "\n",
    "    all_loss.append(loss)\n",
    "    all_score.append(score)\n",
    "\n",
    "    # 모델 폴더가 없다면 생성\n",
    "    if not os.path.exists('model/'):\n",
    "        os.mkdir('model/')\n",
    "    \n",
    "    if (score != 0) and (BEST_HISTORY < score):\n",
    "        BEST_HISTORY = score\n",
    "        torch.save(model, f'model/best_model{epoch}.pth')\n",
    "\n",
    "    if BEST_HISTORY >= score:\n",
    "        stop_num += 1\n",
    "        if stop_num > 10 :\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Test(model, dataset, criterion, device):\n",
    "    model.eval()\n",
    "    losses_t= list()\n",
    "    corrects_t = list()\n",
    "\n",
    "    for images, labels in dataset:\n",
    "        images, labels = images.to(device), labels.to(device).float().unsqueeze(1)\n",
    "\n",
    "        pre_t = model(images)\n",
    "        loss = criterion(pre_t, labels)\n",
    "        losses_t.append(loss.item())\n",
    "        \n",
    "        yhat = torch.sigmoid(pre_t)>.5\n",
    "        corrects_t.extend(torch.eq(yhat, labels).cpu().tolist())\n",
    "        \n",
    "    print(f'Val Loss : {np.mean(losses_t)}, Val Accuracy : {np.mean(corrects_t)}')\n",
    "\n",
    "    return  print(max(corrects_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (100,) and (11,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m fg, axes\u001b[38;5;241m=\u001b[39mplt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m5\u001b[39m), sharex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m )\n\u001b[1;32m----> 2\u001b[0m \u001b[43maxes\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mVal\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m axes[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mgrid()\n\u001b[0;32m      4\u001b[0m axes[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mlegend()\n",
      "File \u001b[1;32mc:\\Users\\KDP-23\\anaconda3\\envs\\TORCH_CV_38\\lib\\site-packages\\matplotlib\\axes\\_axes.py:1688\u001b[0m, in \u001b[0;36mAxes.plot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1445\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1446\u001b[0m \u001b[38;5;124;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[0;32m   1447\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1685\u001b[0m \u001b[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[0;32m   1686\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1687\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[38;5;241m.\u001b[39mLine2D)\n\u001b[1;32m-> 1688\u001b[0m lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_lines(\u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39mdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)]\n\u001b[0;32m   1689\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[0;32m   1690\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_line(line)\n",
      "File \u001b[1;32mc:\\Users\\KDP-23\\anaconda3\\envs\\TORCH_CV_38\\lib\\site-packages\\matplotlib\\axes\\_base.py:311\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[1;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m     this \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    310\u001b[0m     args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plot_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\KDP-23\\anaconda3\\envs\\TORCH_CV_38\\lib\\site-packages\\matplotlib\\axes\\_base.py:504\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[1;34m(self, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[0;32m    501\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes\u001b[38;5;241m.\u001b[39myaxis\u001b[38;5;241m.\u001b[39mupdate_units(y)\n\u001b[0;32m    503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[1;32m--> 504\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y must have same first dimension, but \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    505\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhave shapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    507\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y can be no greater than 2D, but have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    508\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (100,) and (11,)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fg, axes=plt.subplots(1,2, figsize=(10,5), sharex=True )\n",
    "axes[0].plot(range(1, epochs+1), all_loss, label='Val')\n",
    "axes[0].grid()\n",
    "axes[0].legend()\n",
    "axes[0].set_xlabel(\"Epoch\")\n",
    "axes[0].set_ylabel(\"Loss\")\n",
    "axes[0].set_title(\"LOSS\")\n",
    "\n",
    "axes[1].plot(range(1, epochs+1), all_score, label='Val')\n",
    "axes[1].grid()\n",
    "axes[1].legend()\n",
    "axes[1].set_xlabel(\"Epoch\")\n",
    "axes[1].set_ylabel(\"score\")\n",
    "axes[1].set_title(\"score\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TORCH_CV_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 사용자 정의 모델 클래스\n",
    "- 부모클래스 : nn.Module\n",
    "- 필수오버라이딩 : \n",
    "    * __init__() : 모델 층 구성 즉, 설계\n",
    "    * forward() : 순방 향학습 진행 코드 구현 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모듈 로딩\n",
    "\n",
    "import torch                                    # 텐서 관련 모듈\n",
    "import torch.nn as nn                           # 인공신경망 관련 모듈\n",
    "import torch.nn.functional as F                 # 손실함수 활성화함수 등 인공신경망 관련 함수 모듈\n",
    "import torch.optim as optim                     # 최적화 관련 모듈(가중치, 절편 빠르게 찾아주는 알고리즘 (경사하강법))\n",
    "from torchinfo import summary                   # 모델 구조 및 정보 관련 모듈\n",
    "from torchmetrics.regression import *           # 회귀 성능 지표 관련 모듈 \n",
    "from torchmetrics.classification import *       # 분류 성능 지표 관련 모듈\n",
    "\n",
    "import pandas as pd                 # 데이터 파일 분석 관련 모듈\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 랜덤 고정\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# 텐서 저장 및 실행 위치 설정\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [기본] 신경망클래스 <hr>\n",
    "    * 입력층 - 입력 : 피쳐수 고정\n",
    "    * 출력층 - 출력 : 타겟수 고정\n",
    "    * 은닉층 - 일단 고정(요건 내맘대로 설정)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 모델 설계\n",
    "# 사용 데이터셋 : 피쳐 4개, 타겟 1개, [회귀/분류]\n",
    "# 입력층 : 입력 4개    출력 20개    AF ReLU (시그모이드 안쓰는 이유 - 입력한 정보가 소실되어 >>> 좀 보완하기 위해ReLU를 쓰는데 이것도 소실이 있긴하다) > 그 죽은거 회생하는거 ReLU 였지??\n",
    "# 은닉층 : 입력 20개   출력 100개   AF ReLU\n",
    "# 출력층 : 입력 100개  출력 1개     AF X(회귀일땐 필요없음), [Sigmoid or softmax(다중분류) 분류일때 사용 >> 확률로 출력]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 클래스 생성\n",
    "class MyModel(nn.Module): # 부모 클래스 상속 \n",
    "    \n",
    "    # 인스턴스/객체 생성 시 자동 호출되는 메서드\n",
    "    def __init__(self):\n",
    "        # 부모클래스 생성\n",
    "        super().__init__()\n",
    "        # 자식 클래스의 인스턴스 속성 설정\n",
    "        self.input_layer = nn.Linear(4,20)       # W 4 + b 1 => 1P , 5 * 20 = 100개 변수, 지금 현재 4개로 설정\n",
    "        self.hidden_layer = nn.Linear(20,100)    # W 20 + b 1 => 1P , 21 * 100 = 2100개 변수\n",
    "        self.output_layer = nn.Linear(100,1)     # W 100 + b 1 => 1P , 101 * 1 = 101개 변수\n",
    "\n",
    "    # ---------------------------------------------------------------- 여기까지는 모델 구성 설계까지만\n",
    "\n",
    "    # 순반향/전방향 학습 진행 시 자동호출되는 메서드 (콜백함수 Callback func : 시스템에서 호출되는 함수)\n",
    "    # 전달 인자 : 학습용 데이터셋 \n",
    "    def forward(self, x):\n",
    "        print('calling forward()')\n",
    "        y=self.input_layer(x) # 1개 퍼셉트론 : y= x1W1 + x2W2 + x3W3 + x4W4 + b * 20개 [피쳐,가중치(W)] 4개 절편(b)1개\n",
    "        y= F.relu(y)  # relu의 결과는  0 <= y --> relu도 완벽하지않음 데이터 소실 적다는거지 없다는건 아님 >> 죽은 relu 발생  ==> 보완 >> leakyReLu\n",
    "\n",
    "        y=self.hidden_layer(y) # 1개 퍼셉트론 : y= x1W1 + x2W2 + ... + x20W20 + b * 100개  [피쳐,가중치(W)] 20개 절편(b)1개\n",
    "        y= F.relu(y)\n",
    "        \n",
    "        return self.output_layer(y) # 1개 퍼셉트론 : y= x1W1 + x2W2 + ... + x100W100 + b * 1개  [피쳐,가중치(W)] 100개 절편(b)1개 >> 출력\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 피쳐 수 동적인 모델\n",
    "class MyModel2(nn.Module): # 부모 클래스 상속 \n",
    "    \n",
    "    # 인스턴스/객체 생성 시 자동 호출되는 메서드\n",
    "    def __init__(self, in_feature):\n",
    "        # 부모클래스 생성\n",
    "        super().__init__()\n",
    "        # 자식 클래스의 인스턴스 속성 설정\n",
    "        self.input_layer = nn.Linear(in_feature,20)       # W [in_featur개수] + b 1 => 1P , (n+1) * 20 = 100개 변수, in_featur개수 \n",
    "        self.hidden_layer = nn.Linear(20,100)    # W 20 + b 1 => 1P , 21 * 100 = 2100개 변수\n",
    "        self.output_layer = nn.Linear(100,1)     # W 100 + b 1 => 1P , 101 * 1 = 101개 변수\n",
    "\n",
    "    # ---------------------------------------------------------------- 여기까지는 모델 구성만\n",
    "\n",
    "    # 순반향/전방향 학습 진행 시 자동호출되는 메서드 (콜백함수 Callback func : 시스템에서 호출되는 함수)\n",
    "    # 전달 인자 : 학습용 데이터셋 \n",
    "    def forward(self, x):\n",
    "        print('calling forward()')\n",
    "        y=self.input_layer(x) # 1개 퍼셉트론 : y= x1W1 + x2W2 + x3W3 + x4W4 + b * 20개 [피쳐,가중치(W)] 4개 절편(b)1개\n",
    "        y= F.relu(y)  # relu의 결과는  0 <= y \n",
    "\n",
    "        y=self.hidden_layer(y) # 1개 퍼셉트론 : y= x1W1 + x2W2 + ... + x20W20 + b * 100개  [피쳐,가중치(W)] 20개 절편(b)1개\n",
    "        y= F.relu(y)\n",
    "        \n",
    "        return self.output_layer(y) # 1개 퍼셉트론 : y= x1W1 + x2W2 + ... + x100W100 + b * 1개  [피쳐,가중치(W)] 100개 절편(b)1개 >> 출력\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 피쳐 수, 은닉층 퍼셉트론 수 동적인 모델\n",
    "class MyModel3(nn.Module): # 부모 클래스 상속 \n",
    "    \n",
    "    # 인스턴스/객체 생성 시 자동 호출되는 메서드\n",
    "    def __init__(self, in_feature, in_out, h_out):\n",
    "        # 부모클래스 생성\n",
    "        super().__init__()\n",
    "        # 자식 클래스의 인스턴스 속성 설정\n",
    "        self.input_layer = nn.Linear(in_feature,in_out)     \n",
    "        self.hidden_layer = nn.Linear(in_out,h_out)  \n",
    "        self.output_layer = nn.Linear(h_out,1)   \n",
    "\n",
    "    # ---------------------------------------------------------------- 여기까지는 모델 구성만\n",
    "\n",
    "    # 순반향/전방향 학습 진행 시 자동호출되는 메서드 (콜백함수 Callback func : 시스템에서 호출되는 함수)\n",
    "    # 전달 인자 : 학습용 데이터셋 \n",
    "    def forward(self, x):\n",
    "        print('calling forward()')\n",
    "        y=self.input_layer(x) \n",
    "        y= F.relu(y)  \n",
    "\n",
    "        y=self.hidden_layer(y) \n",
    "        y= F.relu(y)\n",
    "        \n",
    "        return self.output_layer(y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 은닉층의 개수, 입력 피쳐 수, 은닉층 퍼셉트론 수 동적인 모델 - 나의 도전 ㅋㅋ >>> 정확한건 다음 페이지에\n",
    "class MyModel4(nn.Module): # 부모 클래스 상속 \n",
    "    \n",
    "    # 추가적으로 은닉층 갯수와 각 은닉층의 퍼셉트론의 개수가 있어야한다.\n",
    "    def __init__(self, in_feature, in_out, h_out, *args):\n",
    "        # 부모클래스 생성\n",
    "        super().__init__()\n",
    "        # 자식 클래스의 인스턴스 속성 설정\n",
    "        \n",
    "        # 입력층\n",
    "        self.input_layer = nn.Linear(in_feature,in_out)  \n",
    "        \n",
    "        # 은닉층\n",
    "        if args<=0:\n",
    "            pass\n",
    "        else:\n",
    "            for i in range(1, args+1):\n",
    "                print(f'은닉{i}층')\n",
    "                self.hidden_layer = nn.Linear(in_out,h_out)  \n",
    "        \n",
    "        # 출력층\n",
    "        self.output_layer = nn.Linear(h_out,1)   \n",
    "\n",
    "    # ---------------------------------------------------------------- 여기까지는 모델 구성 설계만\n",
    "\n",
    "    # 순반향/전방향 학습 진행 시 자동호출되는 메서드 (콜백함수 Callback func : 시스템에서 호출되는 함수)\n",
    "    # 전달 인자 : 학습용 데이터셋 \n",
    "    def forward(self, x):\n",
    "        print('calling forward()')\n",
    "        y=self.input_layer(x) \n",
    "        y= F.relu(y)  \n",
    "\n",
    "        y=self.hidden_layer(y) \n",
    "        y= F.relu(y)\n",
    "        \n",
    "        return self.output_layer(y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 모델 인스턴스 생성\n",
    "m1 = MyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('input_layer.weight', Parameter containing:\n",
      "tensor([[ 0.2576, -0.2207, -0.0969,  0.2347],\n",
      "        [-0.4707,  0.2999, -0.1029,  0.2544],\n",
      "        [ 0.0695, -0.0612,  0.1387,  0.0247],\n",
      "        [ 0.1826, -0.1949, -0.0365, -0.0450],\n",
      "        [ 0.0725, -0.0020,  0.4371,  0.1556],\n",
      "        [-0.1862, -0.3020, -0.0838, -0.2157],\n",
      "        [-0.1602,  0.0239,  0.2981,  0.2718],\n",
      "        [-0.4888,  0.3100,  0.1397,  0.4743],\n",
      "        [ 0.3300, -0.4556, -0.4754, -0.2412],\n",
      "        [ 0.4391, -0.0833,  0.2140, -0.2324],\n",
      "        [ 0.4906, -0.2115,  0.3750,  0.0059],\n",
      "        [-0.2634,  0.2570, -0.2654,  0.1471],\n",
      "        [-0.1444, -0.0548, -0.4807, -0.2384],\n",
      "        [ 0.2713, -0.1215,  0.4980,  0.4008],\n",
      "        [-0.0234, -0.3337,  0.3045,  0.1552],\n",
      "        [-0.3232,  0.3248,  0.3036,  0.4434],\n",
      "        [-0.2803, -0.0823, -0.0097,  0.0730],\n",
      "        [-0.3795, -0.3548,  0.2720, -0.1172],\n",
      "        [ 0.2442,  0.0285,  0.1642,  0.1099],\n",
      "        [ 0.1818,  0.2479, -0.4631,  0.2517]], requires_grad=True))\n",
      "('input_layer.bias', Parameter containing:\n",
      "tensor([-0.3516, -0.3773,  0.0304, -0.0852,  0.2937, -0.2896, -0.4445,  0.3639,\n",
      "        -0.0741,  0.2812,  0.1607, -0.3749,  0.1004,  0.1201, -0.3348, -0.2372,\n",
      "         0.1705,  0.0896, -0.2127, -0.1514], requires_grad=True))\n",
      "('hidden_layer.weight', Parameter containing:\n",
      "tensor([[ 0.2048, -0.0414,  0.1261,  ...,  0.1389, -0.1618, -0.1610],\n",
      "        [-0.1352,  0.0281,  0.2229,  ...,  0.0578, -0.1529, -0.1878],\n",
      "        [-0.1025, -0.0260, -0.1371,  ..., -0.0956,  0.1590, -0.0731],\n",
      "        ...,\n",
      "        [-0.1844, -0.1471, -0.1923,  ...,  0.1118,  0.0691,  0.0738],\n",
      "        [-0.2086, -0.0753, -0.1438,  ...,  0.1593, -0.0429,  0.0194],\n",
      "        [-0.1867,  0.1260,  0.0129,  ...,  0.0369, -0.1090, -0.0607]],\n",
      "       requires_grad=True))\n",
      "('hidden_layer.bias', Parameter containing:\n",
      "tensor([-0.0588,  0.1044,  0.1574,  0.1782, -0.1521, -0.0239,  0.0525, -0.0874,\n",
      "         0.1470, -0.0249,  0.0239, -0.1912,  0.1616, -0.1482, -0.0054, -0.1824,\n",
      "         0.1701,  0.0129,  0.1007, -0.1758,  0.0190,  0.0916,  0.1579,  0.1643,\n",
      "        -0.0399, -0.0227,  0.0743, -0.0804, -0.0910, -0.2059, -0.2193, -0.1177,\n",
      "         0.0279,  0.0180,  0.0179,  0.0335, -0.0413, -0.2209,  0.1997, -0.1636,\n",
      "        -0.0026,  0.0888,  0.0573, -0.0971, -0.1943,  0.1790,  0.0064,  0.2039,\n",
      "         0.1043,  0.0955,  0.0484, -0.0504, -0.1311,  0.0272,  0.2149,  0.1341,\n",
      "         0.2031,  0.2097,  0.1747, -0.1255, -0.1218, -0.0077, -0.1156, -0.0607,\n",
      "        -0.0766,  0.1957, -0.0150,  0.2164, -0.1734, -0.2206,  0.0320,  0.0446,\n",
      "        -0.0902, -0.1132,  0.0637,  0.0112, -0.0300,  0.1135, -0.0951,  0.1759,\n",
      "        -0.1525, -0.1164,  0.0774,  0.1374, -0.0027, -0.1755, -0.0089, -0.0755,\n",
      "        -0.0714, -0.0413,  0.0538, -0.0520, -0.1568, -0.1116, -0.1873, -0.2233,\n",
      "         0.1081,  0.1249,  0.1690,  0.0177], requires_grad=True))\n",
      "('output_layer.weight', Parameter containing:\n",
      "tensor([[-0.0411, -0.0601,  0.0498, -0.0604,  0.0747, -0.0542, -0.0933, -0.0720,\n",
      "          0.0814,  0.0755,  0.0244,  0.0023,  0.0857,  0.0603,  0.0592, -0.0681,\n",
      "          0.0160,  0.0906, -0.0704,  0.0996, -0.0655, -0.0294, -0.0047,  0.0260,\n",
      "          0.0307, -0.0572,  0.0458,  0.0265, -0.0111, -0.0995,  0.0990, -0.0370,\n",
      "         -0.0533,  0.0990, -0.0507,  0.0668,  0.0610, -0.0651, -0.0806, -0.0544,\n",
      "          0.0313,  0.0943, -0.0056, -0.0299, -0.0288, -0.0696,  0.0999, -0.0011,\n",
      "         -0.0791, -0.0683,  0.0622,  0.0484,  0.0645, -0.0105, -0.0198,  0.0328,\n",
      "         -0.0010, -0.0322,  0.0238, -0.0570, -0.0692, -0.0221, -0.0713,  0.0025,\n",
      "          0.0157,  0.0979,  0.0288, -0.0006, -0.0727, -0.0402, -0.0163, -0.0720,\n",
      "          0.0195,  0.0720,  0.0572, -0.0415, -0.0244,  0.0997, -0.0160, -0.0898,\n",
      "         -0.0009,  0.0051, -0.0372, -0.0109, -0.0815,  0.0308, -0.0694,  0.0344,\n",
      "          0.0753, -0.0646, -0.0392,  0.0096,  0.0002,  0.0586, -0.0060,  0.0162,\n",
      "         -0.0276,  0.0330,  0.0302, -0.0805]], requires_grad=True))\n",
      "('output_layer.bias', Parameter containing:\n",
      "tensor([0.0531], requires_grad=True))\n"
     ]
    }
   ],
   "source": [
    "# 모델 파라미터 즉, W와 b 확인\n",
    "for m in m1.named_parameters(): print(m)\n",
    "\n",
    "   # requires_grad=True 변경이 되는 데이터 > 변수 ,  변경이 되면 안되는 데이터 > 상수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calling forward()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1395],\n",
       "        [-0.1858]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습 진행 ==> 모델 인스턴스(데이터)\n",
    "# 임의의 데이터\n",
    "dataTS = torch.FloatTensor([[1,3,5,7],[2,4,6,8]]) # 넣을려는 데이터의 피쳐의 갯수와 모델의 피쳐의 개수가 일치해야 한다.\n",
    "targetTS= torch.FloatTensor([[4],[5]])\n",
    "\n",
    "# 학습\n",
    "pre_y = m1(dataTS)\n",
    "\n",
    "pre_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyModel3(\n",
       "  (input_layer): Linear(in_features=10, out_features=100, bias=True)\n",
       "  (hidden_layer): Linear(in_features=100, out_features=70, bias=True)\n",
       "  (output_layer): Linear(in_features=70, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m3 = MyModel3(10,100,70)\n",
    "m3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TORCH_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

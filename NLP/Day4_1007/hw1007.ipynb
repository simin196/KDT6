{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] 모듈 로딩 및 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모듈 로딩\n",
    "\n",
    "from Korpora import Korpora     # Open Korean Dataset\n",
    "from konlpy.tag import *        # 형태소 분석기\n",
    "import spacy                    # 형태소 분석기\n",
    "from torch.utils.data import Dataset, DataLoader # Pytorch Dataset관련 모듈\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize, wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
      "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
      "\n",
      "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
      "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
      "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
      "\n",
      "    # Description\n",
      "    Author : e9t@github\n",
      "    Repository : https://github.com/e9t/nsmc\n",
      "    References : www.lucypark.kr/docs/2015-pyconkr/#39\n",
      "\n",
      "    Naver sentiment movie corpus v1.0\n",
      "    This is a movie review dataset in the Korean language.\n",
      "    Reviews were scraped from Naver Movies.\n",
      "\n",
      "    The dataset construction is based on the method noted in\n",
      "    [Large movie review dataset][^1] from Maas et al., 2011.\n",
      "\n",
      "    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/\n",
      "\n",
      "    # License\n",
      "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
      "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
      "\n",
      "[Korpora] Corpus `nsmc` is already installed at C:\\Users\\KDP-23\\Korpora\\nsmc\\ratings_train.txt\n",
      "[Korpora] Corpus `nsmc` is already installed at C:\\Users\\KDP-23\\Korpora\\nsmc\\ratings_test.txt\n"
     ]
    }
   ],
   "source": [
    "## 데이터 로딩\n",
    "nsmc = Korpora.load(\"nsmc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    50000 non-null  object\n",
      " 1   label   50000 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 781.4+ KB\n"
     ]
    }
   ],
   "source": [
    "## test  데이터셋을  DataFrame으로 로딩\n",
    "nsmcDF = pd.DataFrame(nsmc.test)\n",
    "nsmcDF.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 단어 사전을 생성 시 활용\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, feature, label):\n",
    "        self.feature = feature\n",
    "        self.label = label\n",
    "        self.length = feature.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.feature.iloc[index], self.label.iloc[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NSMC의 데이터셋 인스턴스 생성\n",
    "nsmcDS = TextDataset(nsmcDF['text'], nsmcDF['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "굳 ㅋ 1\n"
     ]
    }
   ],
   "source": [
    "for f, l in nsmcDS:\n",
    "    print(f,l)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[2] 데이터 전처리 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 대소문자 통일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list 컴프레이션 사용\n",
    "all_text = (text.lower() for text in nsmcDS) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens=[]\n",
    "for text in all_text:\n",
    "    all_tokens.append(wordpunct_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 불용어 & 구두점 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 불용어 추출\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "eng_stopwords = stopwords.words('english')\n",
    "pun = list(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어, 구두점, 길이 짧은 토큰 삭제\n",
    "for tokenList in all_tokens:\n",
    "    for token in tokenList[::-1]: # 뒤에서 돌린다는게 뭔말인지 아직잘 모르겠음\n",
    "        if token.isnumeric(): # 숫자를 먼저 빼내기위해서 앞에오고 숫자라면 제거 아니면 다음 elif으로 넘어가 조건문으로 들어간다\n",
    "            tokenList.remove(token)\n",
    "        \n",
    "        elif (token in eng_stopwords) or (token in pun) or (len(token) <= 2):\n",
    "            tokenList.remove(token)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tokenList in all_tokens:\n",
    "    print(tokenList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 단어별 빈도수 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰을 키로 해서 빈도수 저장\n",
    "token_freqs = {}\n",
    "\n",
    "# 라인(줄)별 토큰을 읽어서 빈도 체크\n",
    "for tokenList in all_tokens:\n",
    "    for token in tokenList:\n",
    "        # 토튼 키가 존재하지 않으면 키로 추가 후 1로 설정\n",
    "        if token not in token_freqs:\n",
    "            token_freqs[token] = 1\n",
    "        # 이미 존재하는 토큰 키이면 값을 1 증가\n",
    "        else:\n",
    "            token_freqs[token] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = {'a': 1 , 'a': 2}\n",
    "# 'a' in data\n",
    "\n",
    "token_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 빈도별로 몇 개의 단어가 존재하는지 체크 \n",
    "token_freqs.items() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 많이 나온데로 정렬하기 위해\n",
    "## 빈도수별로 단어 또는 단어 개수 저장\n",
    "freqsDict = {}\n",
    "for k, v in token_freqs.items(): # v를 리스트로??\n",
    "    if v not in freqsDict:\n",
    "        # freqsDict[v]= [k] # 단어를 가져갈거면\n",
    "        # freqsDict[v]=1 \n",
    "        freqsDict[v]=[1, [k]]\n",
    "        \n",
    "    else : \n",
    "        # freqsDict[v].append(k) # 개수로 가져갈려면\n",
    "        # freqsDict[v]+=1\n",
    "        freqsDict[v][1].append(k)\n",
    "        freqsDict[v][0] += 1\n",
    "        \n",
    "print(freqsDict)\n",
    "\n",
    "### 빈도수를 보고 이 단어를 지울건지 살릴건지 결정할라고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(freqsDict.items(), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [3] 단어 집합 / 단어 사전 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 빈도가 높게 나오는 순서대로 단어 정렬 => 값을 기준으로 정렬\n",
    "#                   (k, y) --------------  v >>> v로 정렬해줘\n",
    "sortedTokens = sorted(token_freqs.items(), key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 단어사전 생성 및 초기화\n",
    "## 특수 토큰 : 'PAD' , 'OOV' 또는 'UNK'\n",
    "PAD_TOKEN , OOV_TOKEN = 'PAD','OOV'\n",
    "\n",
    "vocab = {PAD_TOKEN:0 , OOV_TOKEN :1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 데이터 코퍼스에서 추출된 토큰(단어)들\n",
    "for idx, tk in enumerate(sortedTokens, 2):\n",
    "    vocab[tk[0]] = idx\n",
    "print(vocab) \n",
    "# 'PAD'[패딩에 사용한 0]: 0, 'OOV'[인코딩안에 없는 단어 인코딩]: 1 \n",
    "                            # OOV 가 많이 있다면 잘못된 단어 사전이라는 것ㅕㅕㅕㅓㅕㅓ을 알수있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 텍스트 문장 ==> 수치화[인코딩] \n",
    "encodingData = []\n",
    "for tokenList in all_tokens:\n",
    "    # 1개 문장 인코딩\n",
    "    sent = []\n",
    "    print(f' 문장 {tokenList}')\n",
    "    for token in tokenList:\n",
    "        sent.append(vocab[token])\n",
    "    # 인코딩 된 문장 저장\n",
    "    encodingData.append(sent)\n",
    "    print(f' ==> 인코딩 {sent}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [4] 패딩 (Padding) <hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장들 마다 길이 체크 >> 문장의 길이를 맞춰줄 계획\n",
    "dataLen = list (len(sent) for sent in encodingData )\n",
    "# dataLen = [len(sent) for sent in encodingData] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 히스토그램을보고 문장 길이 확인\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(dataLen, bins=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 기준 길이\n",
    "MAXLen = max(dataLen)\n",
    "MAXLen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "## [1] 가장 긴 문장 길이 기준\n",
    "for idx, sent in enumerate(encodingData):\n",
    "    cur_len = len(sent)\n",
    "    if cur_len < MAXLen:\n",
    "        # print(cur_len, MAXLen - cur_len ) # 확인용\n",
    "        encodingData[idx] = sent + [0]*(MAXLen - cur_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in encodingData[:3]:\n",
    "    print(len(_), _)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "## [2] 지정한 문장 길이 기준\n",
    "\n",
    "MAXLen = 100\n",
    "\n",
    "for idx, sent in enumerate(encodingData):\n",
    "    cur_len = len(sent)\n",
    "    if cur_len < MAXLen:\n",
    "        encodingData[idx] = sent + ([0]*(MAXLen - cur_len))\n",
    "    else:\n",
    "        # 앞부분 자를 때\n",
    "        encodingData[idx] = sent[cur_len - MAXLen:]\n",
    "\n",
    "        # # 뒷부분 제거 시\n",
    "        # encodingData[idx] = sent[:MAXLen]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxLen = 8\n",
    "a= [1,2,3,4,5,6,7,8,9,0]\n",
    "\n",
    "# 필요한 부분 a = [1,2,3,4,5,6,7,8] : 뒷부분 자르기\n",
    "b = a[:maxLen]\n",
    "\n",
    "# 필요한 부분 a = [3,4,5,6,7,8,9,0]] : 앞부분 자르기\n",
    "c = a[len(a) - maxLen:]\n",
    "\n",
    "b, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TEXT_018",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

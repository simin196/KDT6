{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 문장 분류 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 n_vocab,\n",
    "                 hidden_dim,\n",
    "                 embedding_dim,\n",
    "                 n_layers, \n",
    "                 dropout = 0.5,\n",
    "                 bidirectional = True,\n",
    "                 model_type = 'lstm'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=n_vocab,\n",
    "            embedding_dim = embedding_dim,\n",
    "            padding_idx= 0)\n",
    "        \n",
    "        if model_type =='rnn':\n",
    "            self.model = nn.RNN(\n",
    "                input_size=embedding_dim,\n",
    "                hidden_size = hidden_dim,\n",
    "                num_layers= n_layers,\n",
    "                bidirectional=bidirectional,\n",
    "                dropout=dropout,\n",
    "                batch_first=True)\n",
    "    \n",
    "        elif model_type == \"lstm\":\n",
    "            self.model = nn.LSTM(\n",
    "                input_size=embedding_dim,\n",
    "                hidden_size = hidden_dim,\n",
    "                num_layers= n_layers,\n",
    "                bidirectional=bidirectional,\n",
    "                dropout=dropout,\n",
    "                batch_first=True )\n",
    "            \n",
    "        if bidirectional:\n",
    "            self.classifier = nn.Linear(hidden_dim * 2, 1)\n",
    "            \n",
    "        else:\n",
    "            self.classifier = nn.Linear(hidden_dim, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeddings =self.embedding(inputs)\n",
    "        output, _ = self.model(embeddings)\n",
    "        last_output = output[:, -1, :]\n",
    "        last_output = self. dropout(last_output)\n",
    "        logits = self. classifier(last_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터셋 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from Korpora import Korpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
      "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
      "\n",
      "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
      "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
      "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
      "\n",
      "    # Description\n",
      "    Author : e9t@github\n",
      "    Repository : https://github.com/e9t/nsmc\n",
      "    References : www.lucypark.kr/docs/2015-pyconkr/#39\n",
      "\n",
      "    Naver sentiment movie corpus v1.0\n",
      "    This is a movie review dataset in the Korean language.\n",
      "    Reviews were scraped from Naver Movies.\n",
      "\n",
      "    The dataset construction is based on the method noted in\n",
      "    [Large movie review dataset][^1] from Maas et al., 2011.\n",
      "\n",
      "    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/\n",
      "\n",
      "    # License\n",
      "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
      "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
      "\n",
      "[Korpora] Corpus `nsmc` is already installed at C:\\Users\\KDP-23\\Korpora\\nsmc\\ratings_train.txt\n",
      "[Korpora] Corpus `nsmc` is already installed at C:\\Users\\KDP-23\\Korpora\\nsmc\\ratings_test.txt\n"
     ]
    }
   ],
   "source": [
    "corpus = Korpora.load(\"nsmc\")\n",
    "corpusDF = pd.DataFrame(corpus.test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = corpusDF.sample(frac=0.9, random_state=42)\n",
    "test= corpusDF.drop(train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|       | text                                                                                     |   label |\n",
      "|------:|:-----------------------------------------------------------------------------------------|--------:|\n",
      "| 33553 | 모든 편견을 날려 버리는 가슴 따뜻한 영화. 로버트 드 니로, 필립 세이모어 호프만 영원하라. |       1 |\n",
      "|  9427 | 무한 리메이크의 소재. 감독의 역량은 항상 그 자리에...                                    |       0 |\n",
      "|   199 | 신날 것 없는 애니.                                                                       |       0 |\n",
      "| 12447 | 잔잔 격동                                                                                |       1 |\n",
      "| 39489 | 오랜만에 찾은 주말의 명화의 보석                                                         |       1 |\n"
     ]
    }
   ],
   "source": [
    "print(train.head(5).to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Size :  45000\n",
      "Testing Data Size :  5000\n"
     ]
    }
   ],
   "source": [
    "print('Training Data Size : ', len(train))\n",
    "print('Testing Data Size : ', len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터 토큰화 및 단어사전 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import *\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(corpus, n_vocab, special_tokens):\n",
    "    counter = Counter()\n",
    "    for tokens in corpus:\n",
    "        counter.update(tokens)\n",
    "    vocab = special_tokens\n",
    "    for token, count in counter.most_common(n_vocab):\n",
    "        vocab.append(token)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Okt()\n",
    "train_tokens = [tokenizer.morphs(review) for review in train.text] \n",
    "test_tokens = [tokenizer.morphs(review) for review in test.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab(corpus=train_tokens, n_vocab=5000, special_tokens=[\"<pad>\", \"<unk>\"])\n",
    "token_to_id ={token: idx for idx, token in enumerate(vocab)}\n",
    "id_to_token ={idx: token for idx, token in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', '<unk>', '.', '이', '영화', '의', '..', '가', '에', '...']\n",
      "5002\n"
     ]
    }
   ],
   "source": [
    "print(vocab[:10])\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 정수 인코딩 및 패딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(sequences, max_length, pad_value):\n",
    "    result = list()\n",
    "    for sequence in sequences:\n",
    "        sequence = sequence[:max_length]\n",
    "        pad_length = max_length - len(sequence)\n",
    "        padded_sequence = sequence +[pad_value] * pad_length\n",
    "        result.append(padded_sequence)\n",
    "    return np.asarray(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_id =  token_to_id['<unk>']\n",
    "train_ids = [\n",
    "    [token_to_id.get(token, unk_id) for token in review] for review in train_tokens]\n",
    "\n",
    "test_ids = [\n",
    "    [token_to_id.get(token, unk_id) for token in review] for review in test_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 32\n",
    "pad_id = token_to_id['<pad>']\n",
    "train_ids = pad_sequences(train_ids, max_length, pad_id)\n",
    "test_ids = pad_sequences(test_ids, max_length, pad_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 223 1716   10 4036 2095  193  755    4    2 2330 1031  220   26   13\n",
      " 4839    1    1    1    2    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0]\n",
      "[3307    5 1997  456    8    1 1013 3906    5    1    1   13  223   51\n",
      "    3    1 4684    6    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "print(train_ids[0])\n",
    "print(test_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터로더 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = torch.tensor(train_ids)\n",
    "test_ids = torch.tensor(test_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = torch.tensor(train.label.values, dtype=torch.float32)\n",
    "test_labels = torch.tensor(test.label.values, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(train_ids, train_labels)\n",
    "test_dataset = TensorDataset(test_ids, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_vocab = len(token_to_id)\n",
    "hidden_dim = 64\n",
    "embedding_dim = 128\n",
    "n_layers =2\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else 'cpu'\n",
    "classifier = SentenceClassifier(n_vocab=cnt_vocab, hidden_dim=hidden_dim, embedding_dim=embedding_dim, n_layers=n_layers).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "optimizer = optim.RMSprop(classifier.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss 0 : 0.6880030035972595\n",
      "Train Loss 500 : 0.6941480021514816\n",
      "Train Loss 1000 : 0.688730467449535\n",
      "Train Loss 1500 : 0.6745151617104495\n",
      "Train Loss 2000 : 0.6626327075343439\n",
      "Train Loss 2500 : 0.6530778975021548\n",
      "Val Loss : 0.5756861278161518, Val Accuracy : 0.7074444444444444\n",
      "Train Loss 0 : 0.7166152596473694\n",
      "Train Loss 500 : 0.570579332208443\n",
      "Train Loss 1000 : 0.5619320194799822\n",
      "Train Loss 1500 : 0.5580665143349424\n",
      "Train Loss 2000 : 0.5450983932171983\n",
      "Train Loss 2500 : 0.5335616979514156\n",
      "Val Loss : 0.4245760769944491, Val Accuracy : 0.8066444444444445\n",
      "Train Loss 0 : 0.4047014117240906\n",
      "Train Loss 500 : 0.4467914657678433\n",
      "Train Loss 1000 : 0.4360128542134812\n",
      "Train Loss 1500 : 0.4361931407386108\n",
      "Train Loss 2000 : 0.4320067875664393\n",
      "Train Loss 2500 : 0.4291237010533025\n",
      "Val Loss : 0.3722901458031251, Val Accuracy : 0.8352\n",
      "Train Loss 0 : 0.2805987000465393\n",
      "Train Loss 500 : 0.3820060052855048\n",
      "Train Loss 1000 : 0.39141541018293097\n",
      "Train Loss 1500 : 0.3929347401307393\n",
      "Train Loss 2000 : 0.39061689319862597\n",
      "Train Loss 2500 : 0.388786664861934\n",
      "Val Loss : 0.333374126221345, Val Accuracy : 0.8549777777777777\n",
      "Train Loss 0 : 0.5270004868507385\n",
      "Train Loss 500 : 0.35208943407395166\n",
      "Train Loss 1000 : 0.34718892617629366\n",
      "Train Loss 1500 : 0.35119968663645457\n",
      "Train Loss 2000 : 0.3525678562274997\n",
      "Train Loss 2500 : 0.3520206800494038\n",
      "Val Loss : 0.30151148098714614, Val Accuracy : 0.8664666666666667\n"
     ]
    }
   ],
   "source": [
    "def train(model, dataset, criterion, optimizer, device, interval):\n",
    "    model.train()\n",
    "    losses= list()\n",
    "\n",
    "    for step, (input_ids, labels) in enumerate(dataset):\n",
    "        input_ids = input_ids.to(device)\n",
    "        labels = labels.to(device).unsqueeze(1)\n",
    "\n",
    "        logits = model(input_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % interval ==0:\n",
    "            print(f'Train Loss {step} : {np.mean(losses)}')\n",
    "\n",
    "\n",
    "def test(model, dataset, criterion, device):\n",
    "    model.eval()\n",
    "    losses= list()\n",
    "    corrects = list()\n",
    "\n",
    "    for step, (input_ids, labels) in enumerate(dataset):\n",
    "        input_ids = input_ids.to(device)\n",
    "        labels = labels.to(device).unsqueeze(1)\n",
    "\n",
    "        logits = model(input_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        losses.append(loss.item())\n",
    "        yhat = torch.sigmoid(logits)>.5\n",
    "        corrects.extend(torch.eq(yhat, labels).cpu().tolist())\n",
    "        \n",
    "    print(f'Val Loss : {np.mean(losses)}, Val Accuracy : {np.mean(corrects)}')\n",
    "\n",
    "epochs = 5\n",
    "interval = 500\n",
    "for epoch in range(epochs):\n",
    "    train(classifier, train_loader, criterion, optimizer, device, interval)\n",
    "    test(classifier, train_loader, criterion, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 학습된 모델로부터 임베딩 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "보고싶다 [-0.44044474  1.9632758   0.41487783  0.2231063   0.63580036 -0.54615927\n",
      " -0.7899087  -0.70094746  2.3760195  -0.27559242  0.1922664   0.61469215\n",
      " -1.246909   -0.208986    1.7938346  -1.0357165   0.19075765 -0.11249615\n",
      " -0.53565645 -0.47509468 -0.85269433  1.7697381  -0.39388093  1.9232157\n",
      "  2.1198893   0.9804483  -1.3769224  -3.2657049   1.0480913   1.2467566\n",
      " -1.2173885   0.18797366  0.4779758   0.1686798   1.1648282  -0.8910298\n",
      " -1.7237079  -2.8126023  -1.0814131   0.3408776  -0.18635152  0.20313086\n",
      " -0.04737411 -1.0321995   0.9766377  -1.18542     0.23155864 -0.61769384\n",
      " -0.4495899  -0.22784503 -1.1482393   0.12849554 -0.29890183 -0.91810066\n",
      " -0.13539153 -0.4875324  -0.26028097  0.08725326  0.70289356 -0.5981444\n",
      " -0.7482934  -0.32465085  1.5417975   0.78487426 -1.1451414   0.5749128\n",
      " -2.2202199   1.0255811  -0.38335544  0.42853782  0.21496303  1.5771028\n",
      "  0.7076303   0.28464982  0.7622117  -1.9952075   0.940368   -0.44545466\n",
      "  1.6530398   0.87928736 -0.82973963  0.29890323 -0.47149384 -1.4284052\n",
      " -1.3690575   1.2804027  -1.4084197   0.80048263  0.47961068  1.699851\n",
      "  0.00828639  1.4107845   0.52277076  1.4895117   1.1007876   2.3590932\n",
      "  1.4011002  -0.32827327  0.9963143  -1.2572584  -0.72506773  0.96966654\n",
      "  0.85865617 -0.7259596   0.48008975 -1.1790979  -0.783319    0.11224953\n",
      "  0.22132371 -0.4102699  -0.6013445  -1.0321176  -0.856489   -0.5719518\n",
      "  0.24969472  0.8638191  -0.01848086 -1.4090438  -0.6111046  -0.7987744\n",
      " -0.666529    0.44547102  0.04018855  0.5530194  -0.47128475 -1.2773007\n",
      "  1.2464548  -0.30191204]\n"
     ]
    }
   ],
   "source": [
    "token_to_embedding = dict()\n",
    "embedding_matrix = classifier.embedding.weight.detach().cpu().numpy()\n",
    "\n",
    "for word, emb in zip(vocab, embedding_matrix):\n",
    "    token_to_embedding[word] = emb\n",
    "\n",
    "token = vocab[1000]\n",
    "print( token, token_to_embedding[token])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Ves 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpusDF_1 = pd.DataFrame(corpus.test)\n",
    "tokenizer = Okt()\n",
    "token= [tokenizer.morphs(review) for review in corpusDF_1.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = Word2Vec(\n",
    "    sentences = token,\n",
    "    vector_size =128,\n",
    "    window = 5,\n",
    "    min_count=1,\n",
    "    sg=1,\n",
    "    epochs=3,\n",
    "    max_final_vocab=10000)\n",
    "\n",
    "word2vec.save('../models/word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec= Word2Vec.load('../models/word2vec.model')\n",
    "init_embeddings = np.zeros((cnt_vocab, embedding_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, token in id_to_token.items():\n",
    "    if token not in ['<pad>','<unk>']:\n",
    "        init_embeddings[index] = word2vec.wv[token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = nn.Embedding.from_pretrained(\n",
    "    torch.tensor(init_embeddings, dtype=torch.float32)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceClassifier1(nn.Module):\n",
    "    def __init__(self,\n",
    "                 n_vocab,\n",
    "                 hidden_dim,\n",
    "                 embedding_dim,\n",
    "                 n_layers, \n",
    "                 dropout = 0.5,\n",
    "                 bidirectional = True,\n",
    "                 model_type = 'lstm',\n",
    "                 pretrained_embedding = None):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=n_vocab,\n",
    "            embedding_dim = embedding_dim,\n",
    "            padding_idx= 0)\n",
    "        \n",
    "        if model_type =='rnn':\n",
    "            self.model = nn.RNN(\n",
    "                input_size=embedding_dim,\n",
    "                hidden_size = hidden_dim,\n",
    "                num_layers= n_layers,\n",
    "                bidirectional=bidirectional,\n",
    "                dropout=dropout,\n",
    "                batch_first=True)\n",
    "    \n",
    "        elif model_type == \"lstm\":\n",
    "            self.model = nn.LSTM(\n",
    "                input_size=embedding_dim,\n",
    "                hidden_size = hidden_dim,\n",
    "                num_layers= n_layers,\n",
    "                bidirectional=bidirectional,\n",
    "                dropout=dropout,\n",
    "                batch_first=True )\n",
    "            \n",
    "        if bidirectional:\n",
    "            self.classifier = nn.Linear(hidden_dim * 2, 1)\n",
    "            \n",
    "        else:\n",
    "            self.classifier = nn.Linear(hidden_dim, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        if pretrained_embedding is not None:\n",
    "            self.embedding = nn.Embedding.from_pretrained(\n",
    "                torch.tensor(pretrained_embedding, dtype=torch.float32))\n",
    "        \n",
    "        \n",
    "        else :\n",
    "            self.embedding = nn.Embedding(\n",
    "                num_embeddings=cnt_vocab,\n",
    "                embedding_dim=embedding_dim,\n",
    "                padding_idx= 0)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeddings =self.embedding(inputs)\n",
    "        output, _ = self.model(embeddings)\n",
    "        last_output = output[:, -1, :]\n",
    "        last_output = self. dropout(last_output)\n",
    "        logits = self. classifier(last_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SentenceClassifier1(n_vocab=cnt_vocab, hidden_dim=hidden_dim, embedding_dim=embedding_dim, n_layers=n_layers, pretrained_embedding=init_embeddings).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "optimizer = optim.RMSprop(classifier.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs= 5\n",
    "interval = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss 0 : 0.688694179058075\n",
      "Train Loss 500 : 0.6459737810902967\n",
      "Train Loss 1000 : 0.5867796676172005\n",
      "Train Loss 1500 : 0.5668385048594815\n",
      "Train Loss 2000 : 0.5562820314973786\n",
      "Train Loss 2500 : 0.5466975319247301\n",
      "Val Loss : 0.48883435874382797, Val Accuracy : 0.7687111111111111\n",
      "Train Loss 0 : 0.4954495131969452\n",
      "Train Loss 500 : 0.5033940049464594\n",
      "Train Loss 1000 : 0.4953794547846982\n",
      "Train Loss 1500 : 0.4929087327062329\n",
      "Train Loss 2000 : 0.49022016446406336\n",
      "Train Loss 2500 : 0.4868011046908513\n",
      "Val Loss : 0.45339400471951924, Val Accuracy : 0.7868888888888889\n",
      "Train Loss 0 : 0.3293130099773407\n",
      "Train Loss 500 : 0.4766405849994538\n",
      "Train Loss 1000 : 0.46595476960742865\n",
      "Train Loss 1500 : 0.4640176440480548\n",
      "Train Loss 2000 : 0.4642110054401205\n",
      "Train Loss 2500 : 0.4614615861783739\n",
      "Val Loss : 0.43429906841882643, Val Accuracy : 0.7923111111111111\n",
      "Train Loss 0 : 0.38643699884414673\n",
      "Train Loss 500 : 0.45593318305983993\n",
      "Train Loss 1000 : 0.4515436663851514\n",
      "Train Loss 1500 : 0.4476499797392892\n",
      "Train Loss 2000 : 0.4449272961623367\n",
      "Train Loss 2500 : 0.4415569239732076\n",
      "Val Loss : 0.43187712428252384, Val Accuracy : 0.8020444444444444\n",
      "Train Loss 0 : 0.47736847400665283\n",
      "Train Loss 500 : 0.4299070732054596\n",
      "Train Loss 1000 : 0.43503196812146433\n",
      "Train Loss 1500 : 0.4315429666960025\n",
      "Train Loss 2000 : 0.42872737922604714\n",
      "Train Loss 2500 : 0.42677004697000154\n",
      "Val Loss : 0.4070458450151685, Val Accuracy : 0.8069777777777778\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    train(classifier, train_loader, criterion, optimizer, device, interval)\n",
    "    test(classifier, train_loader, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TEXT_018",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
